{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELV Assignment 1\n",
    "[Reference Paper](https://www.pnas.org/doi/10.1073/pnas.1802705116)  \n",
    "[Reference video 1](https://youtu.be/9GkxEEqEGhg?si=eSbcXl5bgqmUorJJ)  \n",
    "[Reference video 2](https://www.youtube.com/watch?v=hEGGa8y5_wM)  \n",
    "[pythonspeed](https://pythonspeed.com/)\n",
    "\n",
    "\n",
    "![Fig 2 from paper](./figures/fig2.png)\n",
    "\n",
    "\n",
    "* Hard Phases\n",
    "* What is Information theoretically possible\n",
    "* Write about GAMP\n",
    "* LR does'nt have prior info about the fact that the weights are binary unlike the AMP algorithm\n",
    "\n",
    "#### PROBLEMS:\n",
    "* Interpreting different random seed's effect\n",
    "* Why is there not a direct correspondance with the plot from the paper\n",
    "* is regularisation good enough\n",
    "* effect of increasing D\n",
    "* should we try for larger alpha range (to see if there's any phase transition)\n",
    "* consider implementing more optimisations\n",
    "* LR might struggle with high D, why and how to fix\n",
    "\n",
    "### problems to fix:  \n",
    "* kernel crashes at large D\n",
    "* the plots dont match (at D=10^4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import gc\n",
    "\n",
    "def run_experiment(D=10000, teacher_seed=42):\n",
    "    # Step 1: Generate teacher model (with random binary weights)\n",
    "    # we do some things to be able to generate data for large D without memory issues\n",
    "    np.random.seed(teacher_seed) \n",
    "    w_teacher = np.random.choice([1, -1], size=D) # arr of size D with +-1 \n",
    "\n",
    "    # Step 2: Generate dataset\n",
    "    N_total = int(2.2 * D)  # 2D (train) + 0.2D (test)\n",
    "    print('wgenerated')\n",
    "    X = np.random.randn(N_total, D)  \n",
    "    print(X.shape,w_teacher.shape)\n",
    "    y = np.sign(X@(w_teacher))  # labels {+1,-1} \n",
    "    y[y == 0] = 1  # Replace any 0s with 1\n",
    "    del w_teacher\n",
    "    gc.collect()\n",
    "    print('X,y computed')\n",
    "    # comment out \n",
    "    X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "    y = np.memmap('./y_data.dat', dtype='float16', mode='r', shape=(N_total,))\n",
    "    # Split dataset into test and train\n",
    "    X_train, X_test = X[:2*D], X[2*D:]\n",
    "    y_train, y_test = y[:2*D], y[2*D:]\n",
    "    del X, y\n",
    "    gc.collect() \n",
    "    # Define range of α values\n",
    "    print(\"success, test train split done\")\n",
    "    alphas = np.arange(0.2, 2.2, 0.2) # [0.2,0.4...2]\n",
    "    test_errors = []\n",
    "\n",
    "    # Precompute hyperparameter grid for Logistic Regression\n",
    "    Cs = np.logspace(-4, 6, 15) \n",
    "\n",
    "    for alpha in alphas:\n",
    "        n = int(alpha * D)\n",
    "        X_subset = X_train.view()[:n] # prevent array copying\n",
    "        y_subset = y_train.view()[:n]\n",
    "\n",
    "        # Train Logistic Regression with cross-validation\n",
    "        model = LogisticRegressionCV(\n",
    "            Cs=Cs, cv=4, penalty='l2', solver='saga',\n",
    "            max_iter=100000, tol=1e-3, random_state=teacher_seed, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_subset, y_subset)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Compute classification error (misclassification rate)\n",
    "        test_error = np.mean(np.square(y_test-y_pred)) \n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        print(f\"Alpha={alpha:.1f}, Error={test_error:.4f}\")\n",
    "\n",
    "    return alphas, test_errors\n",
    "\n",
    "def plot_results(alphas, errors_list, labels, D,name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for errors, label in zip(errors_list, labels):\n",
    "        plt.plot(alphas, errors, 'o-', label=label)\n",
    "    plt.xlabel('Sample Complexity (α)')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.title(f'Generalization Error vs α (D={D})')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0,)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save figure \n",
    "    filename = f'./figures/generalization_error_D{D}_{name}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved as {filename}\")\n",
    "\n",
    "# Experiment: Run with different teacher seeds\n",
    "# This will fail for large D (especially due to memory issues) -> fix using batches, torch blah blah\n",
    "D = int(1e4) # dimensionality   \n",
    "teacher_seed1,teacher_seed2 = 42,2004\n",
    "\n",
    "#alphas, errors1 = run_experiment(D=D, teacher_seed=teacher_seed1)\n",
    "#_, errors2 = run_experiment(D=D, teacher_seed=teacher_seed2)\n",
    "#plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D,name='combined')\n",
    "#plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D,name = 'teacher 1')\n",
    "#plot_results(alphas, [errors2], [f'seed = {teacher_seed2}'], D=D,name = 'teacher 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import seaborn as sns\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "os.environ[\"MallocStackLogging\"] = \"0\"\n",
    "os.environ[\"MallocStackLoggingNoCompact\"] = \"1\"\n",
    "os.environ[\"DYLD_INSERT_LIBRARIES\"] = \"\"\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# Log resource usage\n",
    "def log_resource_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {memory.percent}% | Available: {memory.available / (1024**3):.2f} GB\")\n",
    "\n",
    "# Calculate batch size\n",
    "def calculate_optimal_batch_size(D):\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    max_memory_usage_gb = available_memory_gb * 0.6  # Use 60% of available memory\n",
    "    estimated_batch_size_gb = D * 2 / (1024**3)  # Assuming float16 (2 bytes per value)\n",
    "    optimal_batch_size = int(max_memory_usage_gb / estimated_batch_size_gb * 1e4)\n",
    "    return max(1000, min(optimal_batch_size, int(1e4)))\n",
    "\n",
    "def calculate_cross_entropy_loss(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Manually compute cross-entropy loss\n",
    "    y_true: binary labels (0 or 1)\n",
    "    y_prob: predicted probabilities of positive class\n",
    "    \"\"\"\n",
    "    # Avoid numerical instability\n",
    "    epsilon = 1e-15\n",
    "    y_prob = np.clip(y_prob, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Binary cross-entropy calculation\n",
    "    loss = -(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n",
    "\n",
    "    # Detailed breakdown\n",
    "    #print(\"\\nManual CE Loss Breakdown:\")\n",
    "    #print(\"Unique true labels:\", np.unique(y_true))\n",
    "    #print(\"Probability range:\", y_prob.min(), y_prob.max())\n",
    "\n",
    "    # Compute individual losses\n",
    "    pos_mask = y_true == 1\n",
    "    neg_mask = y_true == 0\n",
    "\n",
    "    #print(\"Positive class losses:\")\n",
    "    #print(\"  Mean:\", loss[pos_mask].mean())\n",
    "    #print(\"  Min:\", loss[pos_mask].min())\n",
    "    #print(\"  Max:\", loss[pos_mask].max())\n",
    "\n",
    "    #print(\"Negative class losses:\")\n",
    "    #print(\"  Mean:\", loss[neg_mask].mean())\n",
    "    #print(\"  Min:\", loss[neg_mask].min())\n",
    "    #print(\"  Max:\", loss[neg_mask].max())\n",
    "\n",
    "    return loss.mean()\n",
    "# Generate Data\n",
    "def generate_data(D=10000, teacher_seed=42, alpha_min=0.2, alpha_max=2):\n",
    "    try:\n",
    "        print(\"Initializing data generation...\")\n",
    "        key = random.PRNGKey(teacher_seed)\n",
    "        w_teacher = 2 * random.bernoulli(key, 0.5, shape=(D,)) - 1\n",
    "\n",
    "        N_total = int((alpha_max + alpha_min) * D)\n",
    "        print('Creating memory-mapped data...')\n",
    "        X = np.memmap('./X_data.dat', dtype='float16', mode='w+', shape=(N_total, D))\n",
    "        y = np.memmap('./y_data.dat', dtype='float16', mode='w+', shape=(N_total,))\n",
    "\n",
    "        batch_size = calculate_optimal_batch_size(D)\n",
    "        num_batches = (N_total + batch_size - 1) // batch_size\n",
    "        key = random.split(key, num_batches)\n",
    "\n",
    "        @jit\n",
    "        def process_batch(subkey, w_teacher):\n",
    "            X_batch = (random.normal(subkey, (batch_size, D))).astype(jnp.float16)\n",
    "            y_batch = jnp.sign(X_batch @ w_teacher).astype(jnp.float16)\n",
    "            y_batch = jnp.where(y_batch == 0, 1, y_batch)\n",
    "            return X_batch, y_batch\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, N_total)\n",
    "            current_batch_size = end - start\n",
    "            print(f\"Processing batch {start}-{end}\")\n",
    "            X_batch, y_batch = process_batch(key[i], w_teacher)\n",
    "            X[start:end] = np.asarray(X_batch[:current_batch_size])\n",
    "            y[start:end] = np.asarray(y_batch[:current_batch_size])\n",
    "\n",
    "        del w_teacher, key, X_batch, y_batch\n",
    "        gc.collect()\n",
    "        print(\"Data generation completed.\")\n",
    "\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError: Reduce D or batch size.\")\n",
    "        exit()\n",
    "\n",
    "# Run Logistic Regression Experiment\n",
    "\n",
    "def run_experiment2(D=10000, teacher_seed=42, alpha_max=2, alpha_min=0.2, alpha_step=0.2):\n",
    "    try:\n",
    "        N_total = int((alpha_max + alpha_min) * D)\n",
    "        X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "        y = np.memmap('./y_data.dat', dtype='float16', mode='r', shape=(N_total,))\n",
    "\n",
    "        X_train = X[:int(alpha_max * D)]\n",
    "        X_test = X[int(alpha_max * D):]\n",
    "        y_train = y[:int(alpha_max * D)]\n",
    "        y_test = y[int(alpha_max * D):]\n",
    "\n",
    "        alphas = np.arange(alpha_min, alpha_max + alpha_step, alpha_step)\n",
    "        test_errors = []\n",
    "\n",
    "        Cs = np.logspace(np.log10(D)-1,np.log10(D)+1,10)\n",
    "        for alpha in alphas:\n",
    "            n = int(alpha * D)\n",
    "            X_subset = X_train[:n]# * jnp.sqrt(1/D) # making variance 1/D\n",
    "            y_subset = y_train[:n]\n",
    "\n",
    "            # Print subset information\n",
    "\n",
    "            model = LogisticRegressionCV(\n",
    "                Cs=Cs/alpha,\n",
    "                cv=3,\n",
    "                penalty='l2',\n",
    "                solver='saga',\n",
    "                max_iter=int(1e5),\n",
    "                tol=1e-8,\n",
    "                random_state=teacher_seed,\n",
    "            )\n",
    "\n",
    "            model.fit(X_subset, y_subset)\n",
    "\n",
    "            # Detailed model diagnostics\n",
    "            print(\"Norm Square of coefficients:\", (np.linalg.norm(model.coef_))**2)\n",
    "            print(\"Coefficient min/max:\", model.coef_.min(), model.coef_.max())\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            prob_pred = model.predict_proba(X_test)\n",
    "\n",
    "            # Detailed prediction analysis\n",
    "            print(\"Min probability:\", prob_pred.min())\n",
    "            print(\"Max probability:\", prob_pred.max())\n",
    "            print(\"Mean probability of positive class:\", prob_pred[:, 1].mean())\n",
    "\n",
    "            z = model.decision_function(X_test)\n",
    "            print(\"Decision function z: \")\n",
    "            print(f\"Min z: {np.min(z)}\")\n",
    "            print(f\"Max z: {np.max(z)}\")\n",
    "            print(f\"Mean z: {np.mean(z)}\")\n",
    "            print(f\"Standard Deviation of z: {np.std(z)}\")\n",
    "\n",
    "\n",
    "            test_error = np.mean(np.square(y_test - y_pred))\n",
    "            test_errors.append(float(test_error))\n",
    "\n",
    "            # More detailed loss calculation\n",
    "            y_binary = (y_test + 1) // 2\n",
    "            w = model.coef_.flatten()\n",
    "            regularization_term = (1/(2*model.C_[0]))*np.linalg.norm(w)**2\n",
    "            y_binary = (y_test + 1) // 2\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            cross_entropy_loss = calculate_cross_entropy_loss(y_binary,y_prob)\n",
    "            total_loss = regularization_term + cross_entropy_loss\n",
    "\n",
    "            print(f\"Alpha: {alpha:3g} | Optimal C: {model.C_[0]:4g} | Error: {test_error:3g}\")\n",
    "            print(f\"(1/2C)||w||^2 (Regularization Term): {regularization_term:.6g} | CE term: {cross_entropy_loss:6g}\")\n",
    "            print(f\"Regularization Contribution (%): {100 * regularization_term / total_loss:.2f}%\")\n",
    "            print()\n",
    "            sns.histplot(prob_pred[:, 1][y_test == 1], color='blue', label='Positive Class', stat='count',alpha=0.5)\n",
    "            sns.histplot(prob_pred[:, 1][y_test == -1], color='red', label='Negative Class', stat='count',alpha=0.5)\n",
    "            plt.title('Probability Distribution for Positive and Negative Classes')\n",
    "            plt.xlabel('Predicted Probability')\n",
    "            plt.ylabel('Counts')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            del y_binary,w,y_prob\n",
    "            gc.collect()\n",
    "\n",
    "        return alphas, test_errors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], []\n",
    "\n",
    "def print_file_info(file_path):\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size_bytes = os.path.getsize(file_path)\n",
    "        file_size_gb = file_size_bytes / (1024 ** 3)\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Size: ({file_size_gb:.4f} GB)\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Plot Results\n",
    "def plot_results(alphas, errors_list, labels, D, name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for errors, label in zip(errors_list, labels):\n",
    "        plt.plot(alphas, errors, 'o-', label=label)\n",
    "    plt.xlabel('Sample Complexity (α)')\n",
    "    plt.ylabel('Generalization Error (MSE)')\n",
    "    plt.title(f'Generalization Error vs α (D={D})')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, )\n",
    "    plt.legend()\n",
    "\n",
    "    os.makedirs('./figures', exist_ok=True)\n",
    "    filename = f'./figures/generalization_error_D{D}_{name}.png'\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "    print(f\"Plot saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def calculate_cross_entropy_loss_batched(y_true, y_pred, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss in batches to prevent memory issues.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15 # To prevent log(0)\n",
    "    total_loss = 0.0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch_true = y_true[i:batch_end]\n",
    "        batch_pred = y_pred[i:batch_end]\n",
    "\n",
    "        batch_pred = np.clip(batch_pred, epsilon, 1 - epsilon)\n",
    "        batch_loss = -(batch_true * np.log(batch_pred) + (1 - batch_true) * np.log(1 - batch_pred))\n",
    "        total_loss += np.sum(batch_loss)\n",
    "    \n",
    "    return total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "\n",
    "def batch_predict_proba(model, X, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Perform batched probability prediction to handle large datasets correctly.\n",
    "    \"\"\"\n",
    "    total_samples = len(X)\n",
    "    probas = np.zeros(total_samples)  # Preallocate array for efficiency\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch = X[i:batch_end]\n",
    "        print(f\"Processing batch {i}-{batch_end}\")\n",
    "        probas[i:batch_end] = model.predict_proba(batch)[:, 1]\n",
    "        print(\"probas\",probas)\n",
    "    return probas\n",
    "\n",
    "\n",
    "def batch_predict(model, X, batch_size):\n",
    "    \"\"\"\n",
    "    Perform batched prediction to handle large datasets efficiently.\n",
    "    \"\"\"\n",
    "    total_samples = len(X)\n",
    "    predictions = np.zeros(total_samples, dtype=int)  # Preallocate for efficiency\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch = X[i:batch_end]\n",
    "        predictions[i:batch_end] = model.predict(batch)  # Directly predict {-1,1}\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def optimized_training(D, teacher_seed, alpha_max=2, alpha_min=0.2, alpha_step=0.2, \n",
    "                        n_splits=4, batch_size=4096, num_epochs=5):\n",
    "    \"\"\"Train an SGD classifier using cross-validation with memory-efficient data handling.\"\"\"\n",
    "    \n",
    "    N_total = int((alpha_max + alpha_min) * D)\n",
    "    X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "    y = np.memmap('./y_data.dat', dtype='float16', mode='r', shape=(N_total,))\n",
    "    \n",
    "    train_size = int(alpha_max * D)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    alphas = np.arange(alpha_min, alpha_max+alpha_step, alpha_step) \n",
    "    test_errors = []\n",
    "    reg_values = np.logspace(-4, 2, 15)  # Adjusted regularization range\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)  # **No shuffling**\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_samples = int(alpha * D)\n",
    "        X_subset, y_subset = X_train[:n_samples], y_train[:n_samples]\n",
    "        \n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        best_alpha_value = None\n",
    "        \n",
    "        print(f\"Alpha = {alpha}\")\n",
    "        \n",
    "        for reg_alpha in reg_values:\n",
    "            print(f\"  Regularization alpha = {reg_alpha}\")\n",
    "            avg_loss = 0\n",
    "            \n",
    "            model = SGDClassifier(\n",
    "                loss='log_loss',\n",
    "                penalty='l2',\n",
    "                alpha=reg_alpha,\n",
    "                max_iter=1,  # **1 iteration per partial_fit call**\n",
    "                tol=None,  \n",
    "                random_state=teacher_seed,\n",
    "                warm_start=True,\n",
    "                n_jobs=-1,\n",
    "                learning_rate='optimal'\n",
    "            )\n",
    "            \n",
    "            print(\"  Training model...\")\n",
    "            \n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_subset), start=1):\n",
    "                print(f\"    Fold {fold_idx}/{n_splits}\")\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    for batch_start in range(0, len(train_idx), batch_size):\n",
    "                        batch_end = min(batch_start + batch_size, len(train_idx))\n",
    "                        batch_train_idx = train_idx[batch_start:batch_end]\n",
    "                        \n",
    "                        if len(batch_train_idx) > 0:\n",
    "                            model.partial_fit(\n",
    "                                X_subset[batch_train_idx],\n",
    "                                y_subset[batch_train_idx],\n",
    "                                classes=[-1, 1]\n",
    "                            )\n",
    "\n",
    "                print(\"Computing probabilities of each binary class\")\n",
    "                try:\n",
    "                    prob_pred = batch_predict_proba(model, X_subset[val_idx], batch_size=batch_size)\n",
    "                    val_labels_binary = (y_subset[val_idx] + 1) // 2\n",
    "                    print(\"      Computing validation loss...\")\n",
    "                    loss = calculate_cross_entropy_loss_batched(val_labels_binary, prob_pred, batch_size=batch_size)\n",
    "                    avg_loss += loss / n_splits\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error in validation: {e}\")\n",
    "                    avg_loss = float('inf')\n",
    "                    break\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_alpha_value = reg_alpha\n",
    "                best_model = model\n",
    "        \n",
    "        print(\"  Evaluating on test set...\")\n",
    "        try:\n",
    "            y_test_pred = batch_predict(best_model, X_test, batch_size=batch_size)  \n",
    "            test_error = np.mean((y_test - y_test_pred) ** 2)  # Compute MSE  \n",
    "            test_errors.append(test_error)\n",
    "            print()\n",
    "            print(f\"  Alpha: {alpha:.3g} | Best regularization alpha: {best_alpha_value:.4g} | Test Error: {test_error:.3g}\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in test set evaluation: {e}\")\n",
    "            test_errors.append(float('inf'))\n",
    "        \n",
    "        del model, best_model\n",
    "        gc.collect()\n",
    "    \n",
    "    return alphas, test_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import parallel_backend\n",
    "from tqdm import tqdm\n",
    "parallel_backend(\"loky\")\n",
    "def calculate_cross_entropy_loss_batched(y_true, y_pred, batch_size=1024):\n",
    "    epsilon = 1e-15\n",
    "    total_loss = 0.0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch_true = y_true[i:batch_end]\n",
    "        batch_pred = y_pred[i:batch_end]\n",
    "        \n",
    "        batch_pred = np.clip(batch_pred, epsilon, 1 - epsilon)\n",
    "        batch_loss = -(batch_true * np.log(batch_pred) + (1 - batch_true) * np.log(1 - batch_pred))\n",
    "        total_loss += np.sum(batch_loss)\n",
    "    \n",
    "    return total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "\n",
    "def batch_predict_proba(model, X, batch_size=1024):\n",
    "    total_samples = len(X)\n",
    "    probas = np.zeros(total_samples)\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch = X[i:batch_end]\n",
    "        probas[i:batch_end] = model.predict_proba(batch)[:, 1]\n",
    "    return probas\n",
    "\n",
    "def batch_predict(model, X, batch_size):\n",
    "    total_samples = len(X)\n",
    "    predictions = np.zeros(total_samples, dtype=int)\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch = X[i:batch_end]\n",
    "        predictions[i:batch_end] = model.predict(batch)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def train_model_for_alpha(alpha, D, teacher_seed, X_train, y_train, X_test, y_test, kf, reg_values, batch_size, num_epochs, n_splits):\n",
    "    n_samples = int(alpha * D)\n",
    "    X_subset, y_subset = X_train[:n_samples], y_train[:n_samples]\n",
    "    \n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_alpha_value = None\n",
    "    \n",
    "    for reg_alpha in tqdm(reg_values, desc=f\"Alpha {alpha:.3g}\"):\n",
    "        avg_loss = 0\n",
    "        model = SGDClassifier(\n",
    "            loss='log_loss',\n",
    "            penalty='l2',\n",
    "            alpha=reg_alpha,\n",
    "            max_iter=1,\n",
    "            tol=None,\n",
    "            random_state=teacher_seed,\n",
    "            warm_start=True,\n",
    "            n_jobs=4,\n",
    "            learning_rate='optimal'\n",
    "        )\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_subset):\n",
    "            for epoch in range(num_epochs):\n",
    "                for batch_start in range(0, len(train_idx), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(train_idx))\n",
    "                    batch_train_idx = train_idx[batch_start:batch_end]\n",
    "                    \n",
    "                    if len(batch_train_idx) > 0:\n",
    "                        model.partial_fit(\n",
    "                            X_subset[batch_train_idx],\n",
    "                            y_subset[batch_train_idx],\n",
    "                            classes=[-1, 1]\n",
    "                        )\n",
    "            \n",
    "            prob_pred = batch_predict_proba(model, X_subset[val_idx], batch_size=batch_size)\n",
    "            val_labels_binary = (y_subset[val_idx] + 1) // 2\n",
    "            loss = calculate_cross_entropy_loss_batched(val_labels_binary, prob_pred, batch_size=batch_size)\n",
    "            avg_loss += loss / n_splits\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_alpha_value = reg_alpha\n",
    "            best_model = model\n",
    "    \n",
    "    y_test_pred = batch_predict(best_model, X_test, batch_size=batch_size)\n",
    "    test_error = np.mean((y_test - y_test_pred) ** 2)\n",
    "    print(test_error)\n",
    "    return alpha, best_alpha_value, test_error\n",
    "\n",
    "def optimized_training(D, teacher_seed, alpha_max=2, alpha_min=0.2, alpha_step=0.2, \n",
    "                        n_splits=3, batch_size=4096, num_epochs=5):\n",
    "    N_total = int((alpha_max + alpha_min) * D)\n",
    "    X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "    y = np.memmap('./y_data.dat', dtype='float16', mode='r', shape=(N_total,))\n",
    "    \n",
    "    train_size = int(alpha_max * D)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    alphas = np.arange(alpha_min, alpha_max + alpha_step, alpha_step)\n",
    "    reg_values = np.logspace(-2.5, 0, 10)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "    \n",
    "    results = Parallel(n_jobs=2)(\n",
    "        delayed(train_model_for_alpha)(alpha, D, teacher_seed, X_train, y_train, X_test, y_test, kf, reg_values, batch_size, num_epochs, n_splits)\n",
    "        for alpha in tqdm(alphas, desc=\"Training Progress\")\n",
    "    )\n",
    "    \n",
    "    alphas, best_reg_values, test_errors = zip(*results)\n",
    "    \n",
    "    for alpha, best_reg, error in results:\n",
    "        print(f\"Alpha: {alpha:.3g} | Best regularization alpha: {best_reg:.4g} | Test Error: {error:.3g}\")\n",
    "    \n",
    "    return alphas, test_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = int(1e5)\n",
    "alpha_min,alpha_max,alpha_step = 0.2,2,0.2\n",
    "teacher_seed1, teacher_seed2 = 24,72\n",
    "\n",
    "two_teachers = (D<=1e4) # change this lol \n",
    "\n",
    "#generate_data(D=D, teacher_seed=teacher_seed1,alpha_max=alpha_max,alpha_min=alpha_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ./X_data.dat\n",
      "Size: (1.6391 GB)\n",
      "File: ./y_data.dat\n",
      "Size: (0.0001 GB)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mmap length is greater than file size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m print_file_info(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./y_data.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m alpha_min,alpha_max,alpha_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m----> 5\u001b[0m alphas, errors1 \u001b[38;5;241m=\u001b[39m \u001b[43moptimized_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_seed1\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5e3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(two_teachers):\n\u001b[1;32m      8\u001b[0m     generate_data(D\u001b[38;5;241m=\u001b[39mD, teacher_seed\u001b[38;5;241m=\u001b[39mteacher_seed2)\n",
      "Cell \u001b[0;32mIn[4], line 99\u001b[0m, in \u001b[0;36moptimized_training\u001b[0;34m(D, teacher_seed, alpha_max, alpha_min, alpha_step, n_splits, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimized_training\u001b[39m(D, teacher_seed, alpha_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, alpha_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, alpha_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[1;32m     97\u001b[0m                         n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     98\u001b[0m     N_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((alpha_max \u001b[38;5;241m+\u001b[39m alpha_min) \u001b[38;5;241m*\u001b[39m D)\n\u001b[0;32m---> 99\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./X_data.dat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmemmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./y_data.dat\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(N_total,))\n\u001b[1;32m    102\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(alpha_max \u001b[38;5;241m*\u001b[39m D)\n",
      "File \u001b[0;32m~/Internship/anaconda3/envs/pyoperon/lib/python3.12/site-packages/numpy/core/memmap.py:268\u001b[0m, in \u001b[0;36mmemmap.__new__\u001b[0;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    267\u001b[0m array_offset \u001b[38;5;241m=\u001b[39m offset \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 268\u001b[0m mm \u001b[38;5;241m=\u001b[39m \u001b[43mmmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m ndarray\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(subtype, shape, dtype\u001b[38;5;241m=\u001b[39mdescr, buffer\u001b[38;5;241m=\u001b[39mmm,\n\u001b[1;32m    271\u001b[0m                        offset\u001b[38;5;241m=\u001b[39marray_offset, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mmap \u001b[38;5;241m=\u001b[39m mm\n",
      "\u001b[0;31mValueError\u001b[0m: mmap length is greater than file size"
     ]
    }
   ],
   "source": [
    "print_file_info('./X_data.dat')\n",
    "print_file_info('./y_data.dat')\n",
    "        \n",
    "alpha_min,alpha_max,alpha_step = 0.2,2,0.2\n",
    "alphas, errors1 = optimized_training(D=D, teacher_seed=teacher_seed1,alpha_max=alpha_max,alpha_min=alpha_min,alpha_step=alpha_step,batch_size=int(5e3))\n",
    "\n",
    "if(two_teachers):\n",
    "    generate_data(D=D, teacher_seed=teacher_seed2)\n",
    "    _, errors2 = optimized_training(D=D, teacher_seed=teacher_seed2,alpha_max=alpha_max,alpha_min=alpha_min,alpha_step=alpha_step,batch_size=int(5e3))\n",
    "\n",
    "    plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D, name='combined')\n",
    "    plot_results(alphas, [errors1], [f'Teacher 1, seed = {teacher_seed1}'], D=D, name='teacher 1')\n",
    "    plot_results(alphas, [errors2], [f'Teacher 2, seed = {teacher_seed2}'], D=D, name='teacher 2')\n",
    "else:\n",
    "    plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D, name='plot')\n",
    "\n",
    "#os.remove('X_data.dat')\n",
    "#os.remove('y_data.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoperon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
