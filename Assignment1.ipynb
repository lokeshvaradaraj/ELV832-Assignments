{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELV Assignment 1\n",
    "[Reference Paper](https://www.pnas.org/doi/10.1073/pnas.1802705116)  \n",
    "[Reference video 1](https://youtu.be/9GkxEEqEGhg?si=eSbcXl5bgqmUorJJ)  \n",
    "[Reference video 2](https://www.youtube.com/watch?v=hEGGa8y5_wM)  \n",
    "[pythonspeed](https://pythonspeed.com/)\n",
    "\n",
    "\n",
    "![Fig 2 from paper](fig2.png)\n",
    "\n",
    "\n",
    "* Hard Phases\n",
    "* What is Information theoretically possible\n",
    "* Write about GAMP\n",
    "* LR does'nt have prior info about the fact that the weights are binary unlike the AMP algorithm\n",
    "\n",
    "#### PROBLEMS:\n",
    "* Interpreting different random seed's effect\n",
    "* Why is there not a direct correspondance with the plot from the paper\n",
    "* is regularisation good enough\n",
    "* effect of increasing D\n",
    "* should we try for larger alpha range (to see if there's any phase transition)\n",
    "* consider implementing more optimisations\n",
    "* LR might struggle with high D, why and how to fix\n",
    "\n",
    "### problems to fix:  \n",
    "* kernel crashes at large D\n",
    "* the plots dont match (at D=10^4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wgenerated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e4\u001b[39m) \u001b[38;5;66;03m# dimensionality   \u001b[39;00m\n\u001b[1;32m     77\u001b[0m teacher_seed1,teacher_seed2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m,\u001b[38;5;241m2004\u001b[39m\n\u001b[0;32m---> 79\u001b[0m alphas, errors1 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_seed1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m _, errors2 \u001b[38;5;241m=\u001b[39m run_experiment(D\u001b[38;5;241m=\u001b[39mD, teacher_seed\u001b[38;5;241m=\u001b[39mteacher_seed2)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D,name='combined')\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D,name = 'teacher 1')\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#plot_results(alphas, [errors2], [f'seed = {teacher_seed2}'], D=D,name = 'teacher 2')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(D, teacher_seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m N_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2.2\u001b[39m \u001b[38;5;241m*\u001b[39m D)  \u001b[38;5;66;03m# 2D (train) + 0.2D (test)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwgenerated\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape,w_teacher\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msign(X\u001b[38;5;241m@\u001b[39m(w_teacher))  \u001b[38;5;66;03m# labels {+1,-1} \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import gc\n",
    "\n",
    "def run_experiment(D=10000, teacher_seed=42):\n",
    "    # Step 1: Generate teacher model (with random binary weights)\n",
    "    # we do some things to be able to generate data for large D without memory issues\n",
    "    np.random.seed(teacher_seed) \n",
    "    w_teacher = np.random.choice([1, -1], size=D) # arr of size D with +-1 \n",
    "\n",
    "    # Step 2: Generate dataset\n",
    "    N_total = int(2.2 * D)  # 2D (train) + 0.2D (test)\n",
    "    print('wgenerated')\n",
    "    X = np.random.randn(N_total, D)  \n",
    "    print(X.shape,w_teacher.shape)\n",
    "    y = np.sign(X@(w_teacher))  # labels {+1,-1} \n",
    "    y[y == 0] = 1  # Replace any 0s with 1\n",
    "    del w_teacher\n",
    "    gc.collect()\n",
    "    print('X,y computed')\n",
    "    # Split dataset into test and train\n",
    "    X_train, X_test = X[:2*D], X[2*D:]\n",
    "    y_train, y_test = y[:2*D], y[2*D:]\n",
    "    del X, y\n",
    "    gc.collect() \n",
    "    # Define range of α values\n",
    "    print(\"success, test train split done\")\n",
    "    alphas = np.arange(0.2, 2.2, 0.2) # [0.2,0.4...2]\n",
    "    test_errors = []\n",
    "\n",
    "    # Precompute hyperparameter grid for Logistic Regression\n",
    "    Cs = np.logspace(-4, 6, 15) \n",
    "\n",
    "    for alpha in alphas:\n",
    "        n = int(alpha * D)\n",
    "        X_subset = X_train.view()[:n] # prevent array copying\n",
    "        y_subset = y_train.view()[:n]\n",
    "\n",
    "        # Train Logistic Regression with cross-validation\n",
    "        model = LogisticRegressionCV(\n",
    "            Cs=Cs, cv=4, penalty='l2', solver='saga',\n",
    "            max_iter=100000, tol=1e-3, random_state=teacher_seed, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_subset, y_subset)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Compute classification error (misclassification rate)\n",
    "        test_error = np.mean(np.square(y_test-y_pred)) \n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        #print(f\"Alpha={alpha:.1f}, Error={test_error:.4f}\")\n",
    "\n",
    "    return alphas, test_errors\n",
    "\n",
    "def plot_results(alphas, errors_list, labels, D,name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for errors, label in zip(errors_list, labels):\n",
    "        plt.plot(alphas, errors, 'o-', label=label)\n",
    "    plt.xlabel('Sample Complexity (α)')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.title(f'Generalization Error vs α (D={D})')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0,)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save figure \n",
    "    filename = f'generalization_error_D{D}_{name}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved as {filename}\")\n",
    "\n",
    "# Experiment: Run with different teacher seeds\n",
    "# This will fail for large D (especially due to memory issues) -> fix using batches, torch blah blah\n",
    "D = int(1e4) # dimensionality   \n",
    "teacher_seed1,teacher_seed2 = 42,2004\n",
    "\n",
    "alphas, errors1 = run_experiment(D=D, teacher_seed=teacher_seed1)\n",
    "_, errors2 = run_experiment(D=D, teacher_seed=teacher_seed2)\n",
    "plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D,name='combined')\n",
    "plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D,name = 'teacher 1')\n",
    "plot_results(alphas, [errors2], [f'seed = {teacher_seed2}'], D=D,name = 'teacher 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import seaborn as sns\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Log resource usage\n",
    "def log_resource_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {memory.percent}% | Available: {memory.available / (1024**3):.2f} GB\")\n",
    "\n",
    "# Calculate batch size\n",
    "def calculate_optimal_batch_size(D):\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    max_memory_usage_gb = available_memory_gb * 0.6  # Use 60% of available memory\n",
    "    estimated_batch_size_gb = D * 2 / (1024**3)  # Assuming float32 (2 bytes per value)\n",
    "    optimal_batch_size = int(max_memory_usage_gb / estimated_batch_size_gb * 1e4)\n",
    "    return max(1000, min(optimal_batch_size, int(1e4)))\n",
    "\n",
    "def calculate_cross_entropy_loss(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Manually compute cross-entropy loss\n",
    "    y_true: binary labels (0 or 1)\n",
    "    y_prob: predicted probabilities of positive class\n",
    "    \"\"\"\n",
    "    # Avoid numerical instability\n",
    "    epsilon = 1e-15\n",
    "    y_prob = np.clip(y_prob, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Binary cross-entropy calculation\n",
    "    loss = -(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n",
    "\n",
    "    # Detailed breakdown\n",
    "    #print(\"\\nManual CE Loss Breakdown:\")\n",
    "    #print(\"Unique true labels:\", np.unique(y_true))\n",
    "    #print(\"Probability range:\", y_prob.min(), y_prob.max())\n",
    "\n",
    "    # Compute individual losses\n",
    "    pos_mask = y_true == 1\n",
    "    neg_mask = y_true == 0\n",
    "\n",
    "    #print(\"Positive class losses:\")\n",
    "    #print(\"  Mean:\", loss[pos_mask].mean())\n",
    "    #print(\"  Min:\", loss[pos_mask].min())\n",
    "    #print(\"  Max:\", loss[pos_mask].max())\n",
    "\n",
    "    #print(\"Negative class losses:\")\n",
    "    #print(\"  Mean:\", loss[neg_mask].mean())\n",
    "    #print(\"  Min:\", loss[neg_mask].min())\n",
    "    #print(\"  Max:\", loss[neg_mask].max())\n",
    "\n",
    "    return loss.mean()\n",
    "# Generate Data\n",
    "def generate_data(D=10000, teacher_seed=42, alpha_min=0.2, alpha_max=2):\n",
    "    try:\n",
    "        print(\"Initializing data generation...\")\n",
    "        key = random.PRNGKey(teacher_seed)\n",
    "        w_teacher = 2 * random.bernoulli(key, 0.5, shape=(D,)) - 1\n",
    "\n",
    "        N_total = int((alpha_max + alpha_min) * D)\n",
    "        print('Creating memory-mapped data...')\n",
    "        X = np.memmap('./X_data.dat', dtype='float16', mode='w+', shape=(N_total, D))\n",
    "        y = np.memmap('./y_data.dat', dtype='int8', mode='w+', shape=(N_total,))\n",
    "\n",
    "        batch_size = calculate_optimal_batch_size(D)\n",
    "        num_batches = (N_total + batch_size - 1) // batch_size\n",
    "        key = random.split(key, num_batches)\n",
    "\n",
    "        @jit\n",
    "        def process_batch(subkey, w_teacher):\n",
    "            X_batch = (random.normal(subkey, (batch_size, D))).astype(jnp.float16)\n",
    "            y_batch = jnp.sign(X_batch @ w_teacher)\n",
    "            y_batch = jnp.where(y_batch == 0, 1, y_batch)\n",
    "            return X_batch, y_batch\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, N_total)\n",
    "            current_batch_size = end - start\n",
    "            print(f\"Processing batch {start}-{end}\")\n",
    "            log_resource_usage()\n",
    "            X_batch, y_batch = process_batch(key[i], w_teacher)\n",
    "            X[start:end] = np.asarray(X_batch[:current_batch_size])\n",
    "            y[start:end] = np.asarray(y_batch[:current_batch_size])\n",
    "\n",
    "        del w_teacher, key, X_batch, y_batch\n",
    "        gc.collect()\n",
    "        print(\"Data generation completed.\")\n",
    "\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError: Reduce D or batch size.\")\n",
    "        exit()\n",
    "\n",
    "# Run Logistic Regression Experiment\n",
    "def run_experiment(D=10000, teacher_seed=42, alpha_max=2, alpha_min=0.2, alpha_step=0.2):\n",
    "    try:\n",
    "        N_total = int((alpha_max + alpha_min) * D)\n",
    "        X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "        y = np.memmap('./y_data.dat', dtype='int8', mode='r', shape=(N_total,))\n",
    "\n",
    "        X_train = X[:int(alpha_max * D)]\n",
    "        X_test = X[int(alpha_max * D):]\n",
    "        y_train = y[:int(alpha_max * D)]\n",
    "        y_test = y[int(alpha_max * D):]\n",
    "\n",
    "        alphas = np.arange(alpha_min, alpha_max + alpha_min, alpha_step)\n",
    "        test_errors = []\n",
    "\n",
    "        Cs = np.logspace(np.log10(D)-1,np.log10(D)+1,10)\n",
    "        for alpha in alphas:\n",
    "            n = int(alpha * D)\n",
    "            X_subset = X_train[:n]# * jnp.sqrt(1/D) # making variance 1/D\n",
    "            y_subset = y_train[:n]\n",
    "\n",
    "            # Print subset information\n",
    "\n",
    "            model = LogisticRegressionCV(\n",
    "                Cs=Cs/alpha,\n",
    "                cv=3,\n",
    "                penalty='l2',\n",
    "                solver='saga',\n",
    "                max_iter=int(1e5),\n",
    "                tol=1e-8,\n",
    "                random_state=teacher_seed,\n",
    "            )\n",
    "\n",
    "            model.fit(X_subset, y_subset)\n",
    "\n",
    "            # Detailed model diagnostics\n",
    "            print(\"Norm Square of coefficients:\", (np.linalg.norm(model.coef_))**2)\n",
    "            print(\"Coefficient min/max:\", model.coef_.min(), model.coef_.max())\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            prob_pred = model.predict_proba(X_test)\n",
    "\n",
    "            # Detailed prediction analysis\n",
    "            print(\"Min probability:\", prob_pred.min())\n",
    "            print(\"Max probability:\", prob_pred.max())\n",
    "            print(\"Mean probability of positive class:\", prob_pred[:, 1].mean())\n",
    "\n",
    "            z = model.decision_function(X_test)\n",
    "            print(\"Decision function z: \")\n",
    "            print(f\"Min z: {np.min(z)}\")\n",
    "            print(f\"Max z: {np.max(z)}\")\n",
    "            print(f\"Mean z: {np.mean(z)}\")\n",
    "            print(f\"Standard Deviation of z: {np.std(z)}\")\n",
    "\n",
    "\n",
    "            test_error = np.mean(np.square(y_test - y_pred))\n",
    "            test_errors.append(float(test_error))\n",
    "\n",
    "            # More detailed loss calculation\n",
    "            y_binary = (y_test + 1) // 2\n",
    "            w = model.coef_.flatten()\n",
    "            regularization_term = (1/(2*model.C_[0]))*np.linalg.norm(w)**2\n",
    "            y_binary = (y_test + 1) // 2\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            cross_entropy_loss = calculate_cross_entropy_loss(y_binary,y_prob)\n",
    "            total_loss = regularization_term + cross_entropy_loss\n",
    "\n",
    "            print(f\"Alpha: {alpha:3g} | Optimal C: {model.C_[0]:4g} | Error: {test_error:3g}\")\n",
    "            print(f\"(1/2C)||w||^2 (Regularization Term): {regularization_term:.6g} | CE term: {cross_entropy_loss:6g}\")\n",
    "            print(f\"Regularization Contribution (%): {100 * regularization_term / total_loss:.2f}%\")\n",
    "            print()\n",
    "            sns.histplot(prob_pred[:, 1][y_test == 1], color='blue', label='Positive Class', stat='count',alpha=0.5)\n",
    "            sns.histplot(prob_pred[:, 1][y_test == -1], color='red', label='Negative Class', stat='count',alpha=0.5)\n",
    "            plt.title('Probability Distribution for Positive and Negative Classes')\n",
    "            plt.xlabel('Predicted Probability')\n",
    "            plt.ylabel('Counts')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            del y_binary,w,y_prob\n",
    "            gc.collect()\n",
    "\n",
    "        return alphas, test_errors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], []\n",
    "\n",
    "def print_file_info(file_path):\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size_bytes = os.path.getsize(file_path)\n",
    "        file_size_gb = file_size_bytes / (1024 ** 3)\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Size: ({file_size_gb:.4f} GB)\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Plot Results\n",
    "def plot_results(alphas, errors_list, labels, D, name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for errors, label in zip(errors_list, labels):\n",
    "        plt.plot(alphas, errors, 'o-', label=label)\n",
    "    plt.xlabel('Sample Complexity (α)')\n",
    "    plt.ylabel('Generalization Error (MSE)')\n",
    "    plt.title(f'Generalization Error vs α (D={D})')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, )\n",
    "    plt.legend()\n",
    "\n",
    "    os.makedirs('./figures', exist_ok=True)\n",
    "    filename = f'./figures/generalization_error_D{D}_{name}.png'\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "    print(f\"Plot saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def calculate_cross_entropy_loss_batched(y_true, y_pred, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss in batches to prevent memory issues.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-10  # To prevent log(0)\n",
    "    total_loss = 0.0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch_true = y_true[i:batch_end]\n",
    "        batch_pred = y_pred[i:batch_end]\n",
    "\n",
    "        batch_pred = np.clip(batch_pred, epsilon, 1 - epsilon)\n",
    "        batch_loss = -(batch_true * np.log(batch_pred) + (1 - batch_true) * np.log(1 - batch_pred))\n",
    "        total_loss += np.sum(batch_loss)\n",
    "    \n",
    "    return total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "\n",
    "def batch_predict_proba(model, X, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Perform batched probability prediction to handle large datasets correctly.\n",
    "    \"\"\"\n",
    "    total_samples = len(X)\n",
    "    probas = np.zeros(total_samples)  # Preallocate array for efficiency\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch = X[i:batch_end]\n",
    "        print(f\"Processing batch {i}-{batch_end}\")\n",
    "        \n",
    "        try:\n",
    "            probas[i:batch_end] = model.predict_proba(batch)[:, 1]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}-{batch_end}: {e}\")\n",
    "            probas[i:batch_end] = 0.5  # Default to neutral probability if error occurs\n",
    "    \n",
    "    return probas\n",
    "\n",
    "def optimized_training(D, teacher_seed, alpha_max, alpha_min, alpha_step, n_splits=3, batch_size=4096):\n",
    "    \"\"\"Train an SGD classifier using cross-validation with memory-efficient data handling.\"\"\"\n",
    "    \n",
    "    N_total = int((alpha_max + alpha_min) * D)\n",
    "    X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "    y = np.memmap('./y_data.dat', dtype='int8', mode='r', shape=(N_total,))\n",
    "    \n",
    "    train_size = int(alpha_max * D)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    alphas = np.arange(alpha_min, alpha_max, alpha_step) \n",
    "    test_errors = []\n",
    "    reg_values = np.logspace(-4, 2, 7)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=teacher_seed)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_samples = int(alpha * D)\n",
    "        X_subset, y_subset = X_train[:n_samples], y_train[:n_samples]\n",
    "        \n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        best_alpha_value = None\n",
    "        \n",
    "        print(f\"Alpha = {alpha}\")\n",
    "        \n",
    "        for reg_alpha in reg_values:\n",
    "            print(f\"  Regularization alpha = {reg_alpha}\")\n",
    "            avg_loss = 0\n",
    "            \n",
    "            model = SGDClassifier(\n",
    "                loss='log_loss',\n",
    "                penalty='l2',\n",
    "                alpha=reg_alpha,\n",
    "                max_iter=50,\n",
    "                tol=1e-3,\n",
    "                random_state=teacher_seed,\n",
    "                warm_start=True,\n",
    "                n_jobs=-1,\n",
    "                learning_rate='optimal'\n",
    "            )\n",
    "            \n",
    "            print(\"  Training model...\")\n",
    "            \n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_subset), start=1):\n",
    "                print(f\"    Fold {fold_idx}/{n_splits}\")\n",
    "                \n",
    "                for batch_start in range(0, len(train_idx), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(train_idx))\n",
    "                    batch_train_idx = train_idx[batch_start:batch_end]\n",
    "                    \n",
    "                    if len(batch_train_idx) > 0:\n",
    "                        model.partial_fit(\n",
    "                            X_subset[batch_train_idx],\n",
    "                            y_subset[batch_train_idx],\n",
    "                            classes=[-1, 1]\n",
    "                        )\n",
    "                \n",
    "                print(\"Computing probabilities of each binary class\")\n",
    "                try:\n",
    "                    prob_pred = batch_predict_proba(model, X_subset[val_idx], batch_size=batch_size)\n",
    "                    val_labels_binary = (y_subset[val_idx] + 1) // 2\n",
    "                    print(\"      Computing validation loss...\")\n",
    "                    loss = calculate_cross_entropy_loss_batched(val_labels_binary, prob_pred, batch_size=batch_size)\n",
    "                    avg_loss += loss / n_splits\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error in validation: {e}\")\n",
    "                    avg_loss = float('inf')\n",
    "                    break\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_alpha_value = reg_alpha\n",
    "                best_model = model\n",
    "        \n",
    "        print(\"  Evaluating on test set...\")\n",
    "        try:\n",
    "            test_predictions = batch_predict_proba(best_model, X_test, batch_size=batch_size)\n",
    "            test_error = np.mean(np.square(y_test - test_predictions))\n",
    "            test_errors.append(float(test_error))\n",
    "            print(f\"  Alpha: {alpha:.3g} | Best regularization alpha: {best_alpha_value:.4g} | Test Error: {test_error:.3g}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in test set evaluation: {e}\")\n",
    "            test_errors.append(float('inf'))\n",
    "        \n",
    "        del model, best_model\n",
    "        gc.collect()\n",
    "    \n",
    "    return alphas, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def calculate_cross_entropy_loss_batched(y_true, y_pred, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss in batches to prevent memory issues.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-10  # To prevent log(0)\n",
    "    total_loss = 0.0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch_true = y_true[i:batch_end]\n",
    "        batch_pred = y_pred[i:batch_end]\n",
    "\n",
    "        batch_pred = np.clip(batch_pred, epsilon, 1 - epsilon)\n",
    "        batch_loss = -(batch_true * np.log(batch_pred) + (1 - batch_true) * np.log(1 - batch_pred))\n",
    "        total_loss += np.sum(batch_loss)\n",
    "    \n",
    "    return total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "\n",
    "def batch_predict_proba(model, X, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Perform batched probability prediction to handle large datasets correctly.\n",
    "    \"\"\"\n",
    "    total_samples = len(X)\n",
    "    probas = np.zeros(total_samples)  # Preallocate array for efficiency\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch = X[i:batch_end]\n",
    "        \n",
    "        try:\n",
    "            probas[i:batch_end] = model.predict_proba(batch)[:, 1]\n",
    "        except Exception as e:\n",
    "            probas[i:batch_end] = 0.5  # Default to neutral probability if error occurs\n",
    "    \n",
    "    return probas\n",
    "\n",
    "def train_model(alpha, X_filename, y_filename, kf, reg_values, batch_size, teacher_seed, D):\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_alpha_value = None\n",
    "    \n",
    "    X_train = np.memmap(X_filename, dtype='float16', mode='r', shape=(int(alpha * D), D))\n",
    "    y_train = np.memmap(y_filename, dtype='int8', mode='r', shape=(int(alpha * D),))\n",
    "    \n",
    "    for reg_alpha in reg_values:\n",
    "        avg_loss = 0\n",
    "        model = SGDClassifier(\n",
    "            loss='log_loss',\n",
    "            penalty='l2',\n",
    "            alpha=reg_alpha,\n",
    "            max_iter=50,\n",
    "            tol=1e-3,\n",
    "            random_state=teacher_seed,\n",
    "            warm_start=False,  \n",
    "            n_jobs=-1,\n",
    "            learning_rate='optimal'\n",
    "        )\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train), start=1):\n",
    "            model_fold = SGDClassifier(**model.get_params())\n",
    "            \n",
    "            for batch_start in range(0, len(train_idx), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(train_idx))\n",
    "                batch_train_idx = train_idx[batch_start:batch_end]\n",
    "                \n",
    "                if len(batch_train_idx) > 0:\n",
    "                    model_fold.partial_fit(\n",
    "                        X_train[batch_train_idx],\n",
    "                        y_train[batch_train_idx],\n",
    "                        classes=[-1, 1]\n",
    "                    )\n",
    "            \n",
    "            try:\n",
    "                prob_pred = batch_predict_proba(model_fold, X_train[val_idx], batch_size=batch_size)\n",
    "                val_labels_binary = (y_train[val_idx] + 1) // 2\n",
    "                loss = calculate_cross_entropy_loss_batched(val_labels_binary, prob_pred, batch_size=batch_size)\n",
    "                avg_loss += loss / kf.get_n_splits()\n",
    "            except:\n",
    "                avg_loss = float('inf')\n",
    "                break\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_alpha_value = reg_alpha\n",
    "            best_model = model_fold\n",
    "    \n",
    "    return best_model, best_alpha_value\n",
    "\n",
    "def optimized_training(D, teacher_seed, alpha_max, alpha_min, alpha_step, n_splits=3, batch_size=4096):\n",
    "    \"\"\"Train an SGD classifier using cross-validation with memory-efficient data handling.\"\"\"\n",
    "    \n",
    "    N_total = int((alpha_max + alpha_min) * D)\n",
    "    X_filename = './X_data.dat'\n",
    "    y_filename = './y_data.dat'\n",
    "    \n",
    "    alphas = np.arange(alpha_min, alpha_max, alpha_step) \n",
    "    test_errors = []\n",
    "    reg_values = np.logspace(-4, 2, 7)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=teacher_seed)\n",
    "    \n",
    "    results = Parallel(n_jobs=len(alphas))(\n",
    "        delayed(train_model)(alpha, X_filename, y_filename, kf, reg_values, batch_size, teacher_seed, D) \n",
    "        for alpha in alphas\n",
    "    )\n",
    "    \n",
    "    X_test = np.memmap(X_filename, dtype='float16', mode='r', shape=(N_total, D))[int(alpha_max * D):]\n",
    "    y_test = np.memmap(y_filename, dtype='int8', mode='r', shape=(N_total,))[int(alpha_max * D):]\n",
    "    \n",
    "    for (alpha, (best_model, best_alpha_value)) in zip(alphas, results):\n",
    "        try:\n",
    "            test_predictions = batch_predict_proba(best_model, X_test, batch_size=batch_size)\n",
    "            test_error = np.mean(np.square(y_test - test_predictions))\n",
    "            test_errors.append(float(test_error))\n",
    "            print(f\"  Alpha: {alpha:.3g} | Best regularization alpha: {best_alpha_value:.4g} | Test Error: {test_error:.3g}\")\n",
    "        except:\n",
    "            test_errors.append(float('inf'))\n",
    "    \n",
    "    return alphas, test_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = int(1e5)\n",
    "alpha_min,alpha_max,alpha_step = 0.2,2,0.2\n",
    "teacher_seed1, teacher_seed2 = 42, 24\n",
    "\n",
    "two_teachers = (D<=1e4) # change this lol \n",
    "\n",
    "#generate_data(D=D, teacher_seed=teacher_seed1,alpha_max=alpha_max,alpha_min=alpha_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ./X_data.dat\n",
      "Size: (40.9782 GB)\n",
      "File: ./y_data.dat\n",
      "Size: (0.0002 GB)\n",
      "Alpha = 0.2\n",
      "  Regularization alpha = 0.0001\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 0.001\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 0.01\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 0.1\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 1.0\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 10.0\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 100.0\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Evaluating on test set...\n",
      "  Alpha: 0.2 | Best regularization alpha: 1 | Test Error: 1.09\n",
      "Alpha = 0.4\n",
      "  Regularization alpha = 0.0001\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 0.001\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 0.01\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 0.1\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 1.0\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n"
     ]
    }
   ],
   "source": [
    "print_file_info('./X_data.dat')\n",
    "print_file_info('./y_data.dat')\n",
    "\n",
    "alpha_min,alpha_max,alpha_step = 0.2,2,0.2\n",
    "alphas, errors1 = optimized_training(D=D, teacher_seed=teacher_seed1,alpha_max=alpha_max,alpha_min=alpha_min,alpha_step=alpha_step)\n",
    "if(two_teachers):\n",
    "    generate_data(D=D, teacher_seed=teacher_seed2)\n",
    "    _, errors2 = run_experiment(D=D, teacher_seed=teacher_seed2)\n",
    "    \n",
    "    plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D, name='combined')\n",
    "    plot_results(alphas, [errors1], [f'Teacher 1, seed = {teacher_seed1}'], D=D, name='teacher 1')\n",
    "    plot_results(alphas, [errors2], [f'Teacher 2, seed = {teacher_seed2}'], D=D, name='teacher 2')\n",
    "else:\n",
    "    plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D, name='plot')\n",
    "\n",
    "#os.remove('X_data.dat')\n",
    "#os.remove('y_data.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoperon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
