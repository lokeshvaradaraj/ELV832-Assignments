{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELV Assignment 1\n",
    "[Reference Paper](https://www.pnas.org/doi/10.1073/pnas.1802705116)  \n",
    "[Reference video 1](https://youtu.be/9GkxEEqEGhg?si=eSbcXl5bgqmUorJJ)  \n",
    "[Reference video 2](https://www.youtube.com/watch?v=hEGGa8y5_wM)  \n",
    "[pythonspeed](https://pythonspeed.com/)\n",
    "\n",
    "\n",
    "![Fig 2 from paper](fig2.png)\n",
    "\n",
    "\n",
    "* Hard Phases\n",
    "* What is Information theoretically possible\n",
    "* Write about GAMP\n",
    "* LR does'nt have prior info about the fact that the weights are binary unlike the AMP algorithm\n",
    "\n",
    "#### PROBLEMS:\n",
    "* Interpreting different random seed's effect\n",
    "* Why is there not a direct correspondance with the plot from the paper\n",
    "* is regularisation good enough\n",
    "* effect of increasing D\n",
    "* should we try for larger alpha range (to see if there's any phase transition)\n",
    "* consider implementing more optimisations\n",
    "* LR might struggle with high D, why and how to fix\n",
    "\n",
    "### problems to fix:  \n",
    "* kernel crashes at large D\n",
    "* the plots dont match (at D=10^4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wgenerated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e4\u001b[39m) \u001b[38;5;66;03m# dimensionality   \u001b[39;00m\n\u001b[1;32m     77\u001b[0m teacher_seed1,teacher_seed2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m,\u001b[38;5;241m2004\u001b[39m\n\u001b[0;32m---> 79\u001b[0m alphas, errors1 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_seed1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m _, errors2 \u001b[38;5;241m=\u001b[39m run_experiment(D\u001b[38;5;241m=\u001b[39mD, teacher_seed\u001b[38;5;241m=\u001b[39mteacher_seed2)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D,name='combined')\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D,name = 'teacher 1')\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#plot_results(alphas, [errors2], [f'seed = {teacher_seed2}'], D=D,name = 'teacher 2')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(D, teacher_seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m N_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2.2\u001b[39m \u001b[38;5;241m*\u001b[39m D)  \u001b[38;5;66;03m# 2D (train) + 0.2D (test)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwgenerated\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape,w_teacher\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msign(X\u001b[38;5;241m@\u001b[39m(w_teacher))  \u001b[38;5;66;03m# labels {+1,-1} \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import gc\n",
    "\n",
    "def run_experiment(D=10000, teacher_seed=42):\n",
    "    # Step 1: Generate teacher model (with random binary weights)\n",
    "    # we do some things to be able to generate data for large D without memory issues\n",
    "    np.random.seed(teacher_seed) \n",
    "    w_teacher = np.random.choice([1, -1], size=D) # arr of size D with +-1 \n",
    "\n",
    "    # Step 2: Generate dataset\n",
    "    N_total = int(2.2 * D)  # 2D (train) + 0.2D (test)\n",
    "    print('wgenerated')\n",
    "    X = np.random.randn(N_total, D)  \n",
    "    print(X.shape,w_teacher.shape)\n",
    "    y = np.sign(X@(w_teacher))  # labels {+1,-1} \n",
    "    y[y == 0] = 1  # Replace any 0s with 1\n",
    "    del w_teacher\n",
    "    gc.collect()\n",
    "    print('X,y computed')\n",
    "    # Split dataset into test and train\n",
    "    X_train, X_test = X[:2*D], X[2*D:]\n",
    "    y_train, y_test = y[:2*D], y[2*D:]\n",
    "    del X, y\n",
    "    gc.collect() \n",
    "    # Define range of α values\n",
    "    print(\"success, test train split done\")\n",
    "    alphas = np.arange(0.2, 2.2, 0.2) # [0.2,0.4...2]\n",
    "    test_errors = []\n",
    "\n",
    "    # Precompute hyperparameter grid for Logistic Regression\n",
    "    Cs = np.logspace(-4, 6, 15) \n",
    "\n",
    "    for alpha in alphas:\n",
    "        n = int(alpha * D)\n",
    "        X_subset = X_train.view()[:n] # prevent array copying\n",
    "        y_subset = y_train.view()[:n]\n",
    "\n",
    "        # Train Logistic Regression with cross-validation\n",
    "        model = LogisticRegressionCV(\n",
    "            Cs=Cs, cv=4, penalty='l2', solver='saga',\n",
    "            max_iter=100000, tol=1e-3, random_state=teacher_seed, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_subset, y_subset)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Compute classification error (misclassification rate)\n",
    "        test_error = np.mean(np.square(y_test-y_pred)) \n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        #print(f\"Alpha={alpha:.1f}, Error={test_error:.4f}\")\n",
    "\n",
    "    return alphas, test_errors\n",
    "\n",
    "def plot_results(alphas, errors_list, labels, D,name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for errors, label in zip(errors_list, labels):\n",
    "        plt.plot(alphas, errors, 'o-', label=label)\n",
    "    plt.xlabel('Sample Complexity (α)')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.title(f'Generalization Error vs α (D={D})')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0,)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save figure \n",
    "    filename = f'generalization_error_D{D}_{name}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved as {filename}\")\n",
    "\n",
    "# Experiment: Run with different teacher seeds\n",
    "# This will fail for large D (especially due to memory issues) -> fix using batches, torch blah blah\n",
    "D = int(1e4) # dimensionality   \n",
    "teacher_seed1,teacher_seed2 = 42,2004\n",
    "\n",
    "alphas, errors1 = run_experiment(D=D, teacher_seed=teacher_seed1)\n",
    "_, errors2 = run_experiment(D=D, teacher_seed=teacher_seed2)\n",
    "plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D,name='combined')\n",
    "plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D,name = 'teacher 1')\n",
    "plot_results(alphas, [errors2], [f'seed = {teacher_seed2}'], D=D,name = 'teacher 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import seaborn as sns\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Log resource usage\n",
    "def log_resource_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {memory.percent}% | Available: {memory.available / (1024**3):.2f} GB\")\n",
    "\n",
    "# Calculate batch size\n",
    "def calculate_optimal_batch_size(D):\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    max_memory_usage_gb = available_memory_gb * 0.6  # Use 60% of available memory\n",
    "    estimated_batch_size_gb = D * 2 / (1024**3)  # Assuming float32 (2 bytes per value)\n",
    "    optimal_batch_size = int(max_memory_usage_gb / estimated_batch_size_gb * 1e4)\n",
    "    return max(1000, min(optimal_batch_size, int(1e4)))\n",
    "\n",
    "def calculate_cross_entropy_loss(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Manually compute cross-entropy loss\n",
    "    y_true: binary labels (0 or 1)\n",
    "    y_prob: predicted probabilities of positive class\n",
    "    \"\"\"\n",
    "    # Avoid numerical instability\n",
    "    epsilon = 1e-15\n",
    "    y_prob = np.clip(y_prob, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Binary cross-entropy calculation\n",
    "    loss = -(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n",
    "\n",
    "    # Detailed breakdown\n",
    "    #print(\"\\nManual CE Loss Breakdown:\")\n",
    "    #print(\"Unique true labels:\", np.unique(y_true))\n",
    "    #print(\"Probability range:\", y_prob.min(), y_prob.max())\n",
    "\n",
    "    # Compute individual losses\n",
    "    pos_mask = y_true == 1\n",
    "    neg_mask = y_true == 0\n",
    "\n",
    "    #print(\"Positive class losses:\")\n",
    "    #print(\"  Mean:\", loss[pos_mask].mean())\n",
    "    #print(\"  Min:\", loss[pos_mask].min())\n",
    "    #print(\"  Max:\", loss[pos_mask].max())\n",
    "\n",
    "    #print(\"Negative class losses:\")\n",
    "    #print(\"  Mean:\", loss[neg_mask].mean())\n",
    "    #print(\"  Min:\", loss[neg_mask].min())\n",
    "    #print(\"  Max:\", loss[neg_mask].max())\n",
    "\n",
    "    return loss.mean()\n",
    "# Generate Data\n",
    "def generate_data(D=10000, teacher_seed=42, alpha_min=0.2, alpha_max=2):\n",
    "    try:\n",
    "        print(\"Initializing data generation...\")\n",
    "        key = random.PRNGKey(teacher_seed)\n",
    "        w_teacher = 2 * random.bernoulli(key, 0.5, shape=(D,)) - 1\n",
    "\n",
    "        N_total = int((alpha_max + alpha_min) * D)\n",
    "        print('Creating memory-mapped data...')\n",
    "        X = np.memmap('./X_data.dat', dtype='float16', mode='w+', shape=(N_total, D))\n",
    "        y = np.memmap('./y_data.dat', dtype='int8', mode='w+', shape=(N_total,))\n",
    "\n",
    "        batch_size = calculate_optimal_batch_size(D)\n",
    "        num_batches = (N_total + batch_size - 1) // batch_size\n",
    "        key = random.split(key, num_batches)\n",
    "\n",
    "        @jit\n",
    "        def process_batch(subkey, w_teacher):\n",
    "            X_batch = (random.normal(subkey, (batch_size, D))).astype(jnp.float16)\n",
    "            y_batch = jnp.sign(X_batch @ w_teacher)\n",
    "            y_batch = jnp.where(y_batch == 0, 1, y_batch)\n",
    "            return X_batch, y_batch\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, N_total)\n",
    "            current_batch_size = end - start\n",
    "            print(f\"Processing batch {start}-{end}\")\n",
    "            log_resource_usage()\n",
    "            X_batch, y_batch = process_batch(key[i], w_teacher)\n",
    "            X[start:end] = np.asarray(X_batch[:current_batch_size])\n",
    "            y[start:end] = np.asarray(y_batch[:current_batch_size])\n",
    "\n",
    "        del w_teacher, key, X_batch, y_batch\n",
    "        gc.collect()\n",
    "        print(\"Data generation completed.\")\n",
    "\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError: Reduce D or batch size.\")\n",
    "        exit()\n",
    "\n",
    "# Run Logistic Regression Experiment\n",
    "def run_experiment(D=10000, teacher_seed=42, alpha_max=2, alpha_min=0.2, alpha_step=0.2):\n",
    "    try:\n",
    "        N_total = int((alpha_max + alpha_min) * D)\n",
    "        X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "        y = np.memmap('./y_data.dat', dtype='int8', mode='r', shape=(N_total,))\n",
    "\n",
    "        X_train = X[:int(alpha_max * D)]\n",
    "        X_test = X[int(alpha_max * D):]\n",
    "        y_train = y[:int(alpha_max * D)]\n",
    "        y_test = y[int(alpha_max * D):]\n",
    "\n",
    "        alphas = np.arange(alpha_min, alpha_max + alpha_min, alpha_step)\n",
    "        test_errors = []\n",
    "\n",
    "        Cs = np.logspace(np.log10(D)-1,np.log10(D)+1,10)\n",
    "        for alpha in alphas:\n",
    "            n = int(alpha * D)\n",
    "            X_subset = X_train[:n]# * jnp.sqrt(1/D) # making variance 1/D\n",
    "            y_subset = y_train[:n]\n",
    "\n",
    "            # Print subset information\n",
    "\n",
    "            model = LogisticRegressionCV(\n",
    "                Cs=Cs/alpha,\n",
    "                cv=3,\n",
    "                penalty='l2',\n",
    "                solver='saga',\n",
    "                max_iter=int(1e5),\n",
    "                tol=1e-8,\n",
    "                random_state=teacher_seed,\n",
    "            )\n",
    "\n",
    "            model.fit(X_subset, y_subset)\n",
    "\n",
    "            # Detailed model diagnostics\n",
    "            print(\"Norm Square of coefficients:\", (np.linalg.norm(model.coef_))**2)\n",
    "            print(\"Coefficient min/max:\", model.coef_.min(), model.coef_.max())\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            prob_pred = model.predict_proba(X_test)\n",
    "\n",
    "            # Detailed prediction analysis\n",
    "            print(\"Min probability:\", prob_pred.min())\n",
    "            print(\"Max probability:\", prob_pred.max())\n",
    "            print(\"Mean probability of positive class:\", prob_pred[:, 1].mean())\n",
    "\n",
    "            z = model.decision_function(X_test)\n",
    "            print(\"Decision function z: \")\n",
    "            print(f\"Min z: {np.min(z)}\")\n",
    "            print(f\"Max z: {np.max(z)}\")\n",
    "            print(f\"Mean z: {np.mean(z)}\")\n",
    "            print(f\"Standard Deviation of z: {np.std(z)}\")\n",
    "\n",
    "\n",
    "            test_error = np.mean(np.square(y_test - y_pred))\n",
    "            test_errors.append(float(test_error))\n",
    "\n",
    "            # More detailed loss calculation\n",
    "            y_binary = (y_test + 1) // 2\n",
    "            w = model.coef_.flatten()\n",
    "            regularization_term = (1/(2*model.C_[0]))*np.linalg.norm(w)**2\n",
    "            y_binary = (y_test + 1) // 2\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            cross_entropy_loss = calculate_cross_entropy_loss(y_binary,y_prob)\n",
    "            total_loss = regularization_term + cross_entropy_loss\n",
    "\n",
    "            print(f\"Alpha: {alpha:3g} | Optimal C: {model.C_[0]:4g} | Error: {test_error:3g}\")\n",
    "            print(f\"(1/2C)||w||^2 (Regularization Term): {regularization_term:.6g} | CE term: {cross_entropy_loss:6g}\")\n",
    "            print(f\"Regularization Contribution (%): {100 * regularization_term / total_loss:.2f}%\")\n",
    "            print()\n",
    "            sns.histplot(prob_pred[:, 1][y_test == 1], color='blue', label='Positive Class', stat='count',alpha=0.5)\n",
    "            sns.histplot(prob_pred[:, 1][y_test == -1], color='red', label='Negative Class', stat='count',alpha=0.5)\n",
    "            plt.title('Probability Distribution for Positive and Negative Classes')\n",
    "            plt.xlabel('Predicted Probability')\n",
    "            plt.ylabel('Counts')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            del y_binary,w,y_prob\n",
    "            gc.collect()\n",
    "\n",
    "        return alphas, test_errors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], []\n",
    "\n",
    "def print_file_info(file_path):\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size_bytes = os.path.getsize(file_path)\n",
    "        file_size_gb = file_size_bytes / (1024 ** 3)\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Size: ({file_size_gb:.4f} GB)\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Plot Results\n",
    "def plot_results(alphas, errors_list, labels, D, name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for errors, label in zip(errors_list, labels):\n",
    "        plt.plot(alphas, errors, 'o-', label=label)\n",
    "    plt.xlabel('Sample Complexity (α)')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.title(f'Generalization Error vs α (D={D})')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, )\n",
    "    plt.legend()\n",
    "\n",
    "    os.makedirs('./figures', exist_ok=True)\n",
    "    filename = f'./figures/generalization_error_D{D}_{name}.png'\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "    print(f\"Plot saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def calculate_cross_entropy_loss_batched(y_true, y_pred, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss in batches to prevent memory issues.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels\n",
    "        y_pred (np.ndarray): Predicted probabilities\n",
    "        batch_size (int): Size of batches to process\n",
    "    \n",
    "    Returns:\n",
    "        float: Average cross-entropy loss\n",
    "    \"\"\"\n",
    "    epsilon = 1e-10  # To prevent log(0)\n",
    "    total_loss = 0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_true = y_true[i:i+batch_size]\n",
    "        batch_pred = y_pred[i:i+batch_size]\n",
    "        \n",
    "        # Clip probabilities\n",
    "        batch_pred = np.clip(batch_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Compute batch loss\n",
    "        batch_loss = -(\n",
    "            batch_true * np.log(batch_pred) + \n",
    "            (1 - batch_true) * np.log(1 - batch_pred)\n",
    "        )\n",
    "        \n",
    "        total_loss += np.sum(batch_loss)\n",
    "    \n",
    "    return total_loss / total_samples\n",
    "\n",
    "def batch_predict_proba(model, X, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Perform prediction in batches to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained scikit-learn model\n",
    "        X: Input data array\n",
    "        batch_size: Size of batches to process\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of predicted probabilities\n",
    "    \"\"\"\n",
    "    probas = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch = X[i:i+batch_size]\n",
    "        try:\n",
    "            batch_probas = model.predict_proba(batch)[:, 1]\n",
    "            probas.append(batch_probas)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch prediction: {e}\")\n",
    "            # If a batch fails, try with smaller batch size\n",
    "            try:\n",
    "                batch_probas = model.predict_proba(batch[:batch_size//2])[:, 1]\n",
    "                probas.append(batch_probas)\n",
    "            except Exception as e:\n",
    "                print(f\"Fallback batch prediction failed: {e}\")\n",
    "                # Insert NaN or handle as needed\n",
    "                probas.append(np.full(len(batch), np.nan))\n",
    "    \n",
    "    return np.concatenate(probas) if probas else np.array([])\n",
    "\n",
    "def optimized_training(D, teacher_seed, alpha_max, alpha_min, alpha_step, n_splits=3, batch_size=4096):\n",
    "    \"\"\"Train an SGD classifier using cross-validation with memory-efficient data handling.\"\"\"\n",
    "    \n",
    "    # Compute total number of samples\n",
    "    N_total = int((alpha_max + alpha_min) * D)\n",
    "    \n",
    "    # Use memory-efficient loading with reduced precision\n",
    "    X = np.memmap('./X_data.dat', dtype='float16', mode='r', shape=(N_total, D))\n",
    "    y = np.memmap('./y_data.dat', dtype='int8', mode='r', shape=(N_total,))\n",
    "    \n",
    "    # Split dataset into training and test sets\n",
    "    train_size = int(alpha_max * D)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Initialize alpha values and error tracking\n",
    "    alphas = [alpha_max]  # Keeping original single alpha approach\n",
    "    test_errors = []\n",
    "    \n",
    "    # Regularization parameter sweep (reduced range for M1 Pro efficiency)\n",
    "    reg_values = np.logspace(-2, 0, 3)\n",
    "    \n",
    "    # K-Fold cross-validation setup with memory-efficient settings\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=teacher_seed)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_samples = int(alpha * D)\n",
    "        X_subset, y_subset = X_train[:n_samples], y_train[:n_samples]\n",
    "        \n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        best_alpha_value = None\n",
    "        \n",
    "        print(f\"Alpha = {alpha}\")\n",
    "        \n",
    "        for reg_alpha in reg_values:\n",
    "            print(f\"  Regularization alpha = {reg_alpha}\")\n",
    "            avg_loss = 0\n",
    "            \n",
    "            # Refined model initialization for M1 Pro\n",
    "            model = SGDClassifier(\n",
    "                loss='log_loss',\n",
    "                penalty='l2',\n",
    "                alpha=reg_alpha,\n",
    "                max_iter=50,  # Reduced iterations\n",
    "                tol=1e-3,     # Slightly relaxed tolerance\n",
    "                random_state=teacher_seed,\n",
    "                warm_start=True,\n",
    "                n_jobs=-1,    # Use all available cores\n",
    "                learning_rate='optimal'  # Adaptive learning rate\n",
    "            )\n",
    "            \n",
    "            print(\"  Training model...\")\n",
    "            \n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_subset), start=1):\n",
    "                print(f\"    Fold {fold_idx}/{n_splits}\")\n",
    "                \n",
    "                # Process all training samples in batches\n",
    "                for batch_start in range(0, n_samples, batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(train_idx))\n",
    "                    batch_train_idx = train_idx[batch_start:batch_end]\n",
    "                    \n",
    "                    print(f\"      Processing batch: {batch_start} to {batch_end}\")\n",
    "                    \n",
    "                    # Partial fit with current batch\n",
    "                    model.partial_fit(\n",
    "                        X_subset[batch_train_idx],\n",
    "                        y_subset[batch_train_idx],\n",
    "                        classes=[-1, 1]\n",
    "                    )\n",
    "                \n",
    "                # Compute validation loss with batched prediction and loss calculation\n",
    "                print(\"Computing probabilities of each binary class\")\n",
    "                try:\n",
    "                    # Batched prediction of probabilities\n",
    "                    prob_pred = batch_predict_proba(model, X_subset[val_idx], batch_size=batch_size)\n",
    "                    \n",
    "                    # Convert labels to binary (0/1)\n",
    "                    val_labels_binary = (y_subset[val_idx] + 1) // 2\n",
    "                    \n",
    "                    # Batched cross-entropy loss calculation\n",
    "                    print(\"      Computing validation loss...\")\n",
    "                    loss = calculate_cross_entropy_loss_batched(val_labels_binary, prob_pred, batch_size=batch_size)\n",
    "                    avg_loss += loss / n_splits\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error in validation: {e}\")\n",
    "                    avg_loss = float('inf')\n",
    "                    break\n",
    "            \n",
    "            # Update best model if current model performs better\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_alpha_value = reg_alpha\n",
    "                best_model = model\n",
    "        \n",
    "        # Evaluate best model on test set\n",
    "        print(\"  Evaluating on test set...\")\n",
    "        try:\n",
    "            # Use batched prediction for test set\n",
    "            test_predictions = np.concatenate([\n",
    "                model.predict(X_test[i:i+batch_size]) \n",
    "                for i in range(0, len(X_test), batch_size)\n",
    "            ])\n",
    "            \n",
    "            test_error = np.mean(np.square(y_test - test_predictions))\n",
    "            test_errors.append(float(test_error))\n",
    "            \n",
    "            print(f\"  Alpha: {alpha:.3g} | Best regularization alpha: {best_alpha_value:.4g} | Test Error: {test_error:.3g}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in test set evaluation: {e}\")\n",
    "            test_errors.append(float('inf'))\n",
    "        \n",
    "        # Aggressive memory cleanup\n",
    "        del model, best_model\n",
    "        gc.collect()\n",
    "    \n",
    "    return alphas, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = int(1e5)\n",
    "alpha_min,alpha_max,alpha_step = 0.2,2,0.2\n",
    "teacher_seed1, teacher_seed2 = 42, 24\n",
    "\n",
    "two_teachers = (D<=1e4) # change this lol \n",
    "\n",
    "#generate_data(D=D, teacher_seed=teacher_seed1,alpha_max=alpha_max,alpha_min=alpha_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ./X_data.dat\n",
      "Size: (40.9782 GB)\n",
      "File: ./y_data.dat\n",
      "Size: (0.0002 GB)\n",
      "Alpha = 2\n",
      "  Regularization alpha = 0.01\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "      Batch 1/130\n",
      "      Batch 2/130\n",
      "      Batch 3/130\n",
      "      Batch 4/130\n",
      "      Batch 5/130\n",
      "      Batch 6/130\n",
      "      Batch 7/130\n",
      "      Batch 8/130\n",
      "      Batch 9/130\n",
      "      Batch 10/130\n",
      "      Batch 11/130\n",
      "      Batch 12/130\n",
      "      Batch 13/130\n",
      "      Batch 14/130\n",
      "      Batch 15/130\n",
      "      Batch 16/130\n",
      "      Batch 17/130\n",
      "      Batch 18/130\n",
      "      Batch 19/130\n",
      "      Batch 20/130\n",
      "      Batch 21/130\n",
      "      Batch 22/130\n",
      "      Batch 23/130\n",
      "      Batch 24/130\n",
      "      Batch 25/130\n",
      "      Batch 26/130\n",
      "      Batch 27/130\n",
      "      Batch 28/130\n",
      "      Batch 29/130\n",
      "      Batch 30/130\n",
      "      Batch 31/130\n",
      "      Batch 32/130\n",
      "      Batch 33/130\n",
      "      Batch 34/130\n",
      "      Batch 35/130\n",
      "      Batch 36/130\n",
      "      Batch 37/130\n",
      "      Batch 38/130\n",
      "      Batch 39/130\n",
      "      Batch 40/130\n",
      "      Batch 41/130\n",
      "      Batch 42/130\n",
      "      Batch 43/130\n",
      "      Batch 44/130\n",
      "      Batch 45/130\n",
      "      Batch 46/130\n",
      "      Batch 47/130\n",
      "      Batch 48/130\n",
      "      Batch 49/130\n",
      "      Batch 50/130\n",
      "      Batch 51/130\n",
      "      Batch 52/130\n",
      "      Batch 53/130\n",
      "      Batch 54/130\n",
      "      Batch 55/130\n",
      "      Batch 56/130\n",
      "      Batch 57/130\n",
      "      Batch 58/130\n",
      "      Batch 59/130\n",
      "      Batch 60/130\n",
      "      Batch 61/130\n",
      "      Batch 62/130\n",
      "      Batch 63/130\n",
      "      Batch 64/130\n",
      "      Batch 65/130\n",
      "      Batch 66/130\n",
      "      Batch 67/130\n",
      "      Batch 68/130\n",
      "      Batch 69/130\n",
      "      Batch 70/130\n",
      "      Batch 71/130\n",
      "      Batch 72/130\n",
      "      Batch 73/130\n",
      "      Batch 74/130\n",
      "      Batch 75/130\n",
      "      Batch 76/130\n",
      "      Batch 77/130\n",
      "      Batch 78/130\n",
      "      Batch 79/130\n",
      "      Batch 80/130\n",
      "      Batch 81/130\n",
      "      Batch 82/130\n",
      "      Batch 83/130\n",
      "      Batch 84/130\n",
      "      Batch 85/130\n",
      "      Batch 86/130\n",
      "      Batch 87/130\n",
      "      Batch 88/130\n",
      "      Batch 89/130\n",
      "      Batch 90/130\n",
      "      Batch 91/130\n",
      "      Batch 92/130\n",
      "      Batch 93/130\n",
      "      Batch 94/130\n",
      "      Batch 95/130\n",
      "      Batch 96/130\n",
      "      Batch 97/130\n",
      "      Batch 98/130\n",
      "      Batch 99/130\n",
      "      Batch 100/130\n",
      "      Batch 101/130\n",
      "      Batch 102/130\n",
      "      Batch 103/130\n",
      "      Batch 104/130\n",
      "      Batch 105/130\n",
      "      Batch 106/130\n",
      "      Batch 107/130\n",
      "      Batch 108/130\n",
      "      Batch 109/130\n",
      "      Batch 110/130\n",
      "      Batch 111/130\n",
      "      Batch 112/130\n",
      "      Batch 113/130\n",
      "      Batch 114/130\n",
      "      Batch 115/130\n",
      "      Batch 116/130\n",
      "      Batch 117/130\n",
      "      Batch 118/130\n",
      "      Batch 119/130\n",
      "      Batch 120/130\n",
      "      Batch 121/130\n",
      "      Batch 122/130\n",
      "      Batch 123/130\n",
      "      Batch 124/130\n",
      "      Batch 125/130\n",
      "      Batch 126/130\n",
      "      Batch 127/130\n",
      "      Batch 128/130\n",
      "      Batch 129/130\n",
      "      Batch 130/130\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "      Batch 1/130\n",
      "      Batch 2/130\n",
      "      Batch 3/130\n",
      "      Batch 4/130\n",
      "      Batch 5/130\n",
      "      Batch 6/130\n",
      "      Batch 7/130\n",
      "      Batch 8/130\n",
      "      Batch 9/130\n",
      "      Batch 10/130\n",
      "      Batch 11/130\n",
      "      Batch 12/130\n",
      "      Batch 13/130\n",
      "      Batch 14/130\n",
      "      Batch 15/130\n",
      "      Batch 16/130\n",
      "      Batch 17/130\n",
      "      Batch 18/130\n",
      "      Batch 19/130\n",
      "      Batch 20/130\n",
      "      Batch 21/130\n",
      "      Batch 22/130\n",
      "      Batch 23/130\n",
      "      Batch 24/130\n",
      "      Batch 25/130\n",
      "      Batch 26/130\n",
      "      Batch 27/130\n",
      "      Batch 28/130\n",
      "      Batch 29/130\n",
      "      Batch 30/130\n",
      "      Batch 31/130\n",
      "      Batch 32/130\n",
      "      Batch 33/130\n",
      "      Batch 34/130\n",
      "      Batch 35/130\n",
      "      Batch 36/130\n",
      "      Batch 37/130\n",
      "      Batch 38/130\n",
      "      Batch 39/130\n",
      "      Batch 40/130\n",
      "      Batch 41/130\n",
      "      Batch 42/130\n",
      "      Batch 43/130\n",
      "      Batch 44/130\n",
      "      Batch 45/130\n",
      "      Batch 46/130\n",
      "      Batch 47/130\n",
      "      Batch 48/130\n",
      "      Batch 49/130\n",
      "      Batch 50/130\n",
      "      Batch 51/130\n",
      "      Batch 52/130\n",
      "      Batch 53/130\n",
      "      Batch 54/130\n",
      "      Batch 55/130\n",
      "      Batch 56/130\n",
      "      Batch 57/130\n",
      "      Batch 58/130\n",
      "      Batch 59/130\n",
      "      Batch 60/130\n",
      "      Batch 61/130\n",
      "      Batch 62/130\n",
      "      Batch 63/130\n",
      "      Batch 64/130\n",
      "      Batch 65/130\n",
      "      Batch 66/130\n",
      "      Batch 67/130\n",
      "      Batch 68/130\n",
      "      Batch 69/130\n",
      "      Batch 70/130\n",
      "      Batch 71/130\n",
      "      Batch 72/130\n",
      "      Batch 73/130\n",
      "      Batch 74/130\n",
      "      Batch 75/130\n",
      "      Batch 76/130\n",
      "      Batch 77/130\n",
      "      Batch 78/130\n",
      "      Batch 79/130\n",
      "      Batch 80/130\n",
      "      Batch 81/130\n",
      "      Batch 82/130\n",
      "      Batch 83/130\n",
      "      Batch 84/130\n",
      "      Batch 85/130\n",
      "      Batch 86/130\n",
      "      Batch 87/130\n",
      "      Batch 88/130\n",
      "      Batch 89/130\n",
      "      Batch 90/130\n",
      "      Batch 91/130\n",
      "      Batch 92/130\n",
      "      Batch 93/130\n",
      "      Batch 94/130\n",
      "      Batch 95/130\n",
      "      Batch 96/130\n",
      "      Batch 97/130\n",
      "      Batch 98/130\n",
      "      Batch 99/130\n",
      "      Batch 100/130\n",
      "      Batch 101/130\n",
      "      Batch 102/130\n",
      "      Batch 103/130\n",
      "      Batch 104/130\n",
      "      Batch 105/130\n",
      "      Batch 106/130\n",
      "      Batch 107/130\n",
      "      Batch 108/130\n",
      "      Batch 109/130\n",
      "      Batch 110/130\n",
      "      Batch 111/130\n",
      "      Batch 112/130\n",
      "      Batch 113/130\n",
      "      Batch 114/130\n",
      "      Batch 115/130\n",
      "      Batch 116/130\n",
      "      Batch 117/130\n",
      "      Batch 118/130\n",
      "      Batch 119/130\n",
      "      Batch 120/130\n",
      "      Batch 121/130\n",
      "      Batch 122/130\n",
      "      Batch 123/130\n",
      "      Batch 124/130\n",
      "      Batch 125/130\n",
      "      Batch 126/130\n",
      "      Batch 127/130\n",
      "      Batch 128/130\n",
      "      Batch 129/130\n",
      "      Batch 130/130\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "      Batch 1/130\n",
      "      Batch 2/130\n",
      "      Batch 3/130\n",
      "      Batch 4/130\n",
      "      Batch 5/130\n",
      "      Batch 6/130\n",
      "      Batch 7/130\n",
      "      Batch 8/130\n",
      "      Batch 9/130\n",
      "      Batch 10/130\n",
      "      Batch 11/130\n",
      "      Batch 12/130\n",
      "      Batch 13/130\n",
      "      Batch 14/130\n",
      "      Batch 15/130\n",
      "      Batch 16/130\n",
      "      Batch 17/130\n",
      "      Batch 18/130\n",
      "      Batch 19/130\n",
      "      Batch 20/130\n",
      "      Batch 21/130\n",
      "      Batch 22/130\n",
      "      Batch 23/130\n",
      "      Batch 24/130\n",
      "      Batch 25/130\n",
      "      Batch 26/130\n",
      "      Batch 27/130\n",
      "      Batch 28/130\n",
      "      Batch 29/130\n",
      "      Batch 30/130\n",
      "      Batch 31/130\n",
      "      Batch 32/130\n",
      "      Batch 33/130\n",
      "      Batch 34/130\n",
      "      Batch 35/130\n",
      "      Batch 36/130\n",
      "      Batch 37/130\n",
      "      Batch 38/130\n",
      "      Batch 39/130\n",
      "      Batch 40/130\n",
      "      Batch 41/130\n",
      "      Batch 42/130\n",
      "      Batch 43/130\n",
      "      Batch 44/130\n",
      "      Batch 45/130\n",
      "      Batch 46/130\n",
      "      Batch 47/130\n",
      "      Batch 48/130\n",
      "      Batch 49/130\n",
      "      Batch 50/130\n",
      "      Batch 51/130\n",
      "      Batch 52/130\n",
      "      Batch 53/130\n",
      "      Batch 54/130\n",
      "      Batch 55/130\n",
      "      Batch 56/130\n",
      "      Batch 57/130\n",
      "      Batch 58/130\n",
      "      Batch 59/130\n",
      "      Batch 60/130\n",
      "      Batch 61/130\n",
      "      Batch 62/130\n",
      "      Batch 63/130\n",
      "      Batch 64/130\n",
      "      Batch 65/130\n",
      "      Batch 66/130\n",
      "      Batch 67/130\n",
      "      Batch 68/130\n",
      "      Batch 69/130\n",
      "      Batch 70/130\n",
      "      Batch 71/130\n",
      "      Batch 72/130\n",
      "      Batch 73/130\n",
      "      Batch 74/130\n",
      "      Batch 75/130\n",
      "      Batch 76/130\n",
      "      Batch 77/130\n",
      "      Batch 78/130\n",
      "      Batch 79/130\n",
      "      Batch 80/130\n",
      "      Batch 81/130\n",
      "      Batch 82/130\n",
      "      Batch 83/130\n",
      "      Batch 84/130\n",
      "      Batch 85/130\n",
      "      Batch 86/130\n",
      "      Batch 87/130\n",
      "      Batch 88/130\n",
      "      Batch 89/130\n",
      "      Batch 90/130\n",
      "      Batch 91/130\n",
      "      Batch 92/130\n",
      "      Batch 93/130\n",
      "      Batch 94/130\n",
      "      Batch 95/130\n",
      "      Batch 96/130\n",
      "      Batch 97/130\n",
      "      Batch 98/130\n",
      "      Batch 99/130\n",
      "      Batch 100/130\n",
      "      Batch 101/130\n",
      "      Batch 102/130\n",
      "      Batch 103/130\n",
      "      Batch 104/130\n",
      "      Batch 105/130\n",
      "      Batch 106/130\n",
      "      Batch 107/130\n",
      "      Batch 108/130\n",
      "      Batch 109/130\n",
      "      Batch 110/130\n",
      "      Batch 111/130\n",
      "      Batch 112/130\n",
      "      Batch 113/130\n",
      "      Batch 114/130\n",
      "      Batch 115/130\n",
      "      Batch 116/130\n",
      "      Batch 117/130\n",
      "      Batch 118/130\n",
      "      Batch 119/130\n",
      "      Batch 120/130\n",
      "      Batch 121/130\n",
      "      Batch 122/130\n",
      "      Batch 123/130\n",
      "      Batch 124/130\n",
      "      Batch 125/130\n",
      "      Batch 126/130\n",
      "      Batch 127/130\n",
      "      Batch 128/130\n",
      "      Batch 129/130\n",
      "      Batch 130/130\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 0.1\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "      Batch 1/130\n",
      "      Batch 2/130\n",
      "      Batch 3/130\n",
      "      Batch 4/130\n",
      "      Batch 5/130\n",
      "      Batch 6/130\n",
      "      Batch 7/130\n",
      "      Batch 8/130\n",
      "      Batch 9/130\n",
      "      Batch 10/130\n",
      "      Batch 11/130\n",
      "      Batch 12/130\n",
      "      Batch 13/130\n",
      "      Batch 14/130\n",
      "      Batch 15/130\n",
      "      Batch 16/130\n",
      "      Batch 17/130\n",
      "      Batch 18/130\n",
      "      Batch 19/130\n",
      "      Batch 20/130\n",
      "      Batch 21/130\n",
      "      Batch 22/130\n",
      "      Batch 23/130\n",
      "      Batch 24/130\n",
      "      Batch 25/130\n",
      "      Batch 26/130\n",
      "      Batch 27/130\n",
      "      Batch 28/130\n",
      "      Batch 29/130\n",
      "      Batch 30/130\n",
      "      Batch 31/130\n",
      "      Batch 32/130\n",
      "      Batch 33/130\n",
      "      Batch 34/130\n",
      "      Batch 35/130\n",
      "      Batch 36/130\n",
      "      Batch 37/130\n",
      "      Batch 38/130\n",
      "      Batch 39/130\n",
      "      Batch 40/130\n",
      "      Batch 41/130\n",
      "      Batch 42/130\n",
      "      Batch 43/130\n",
      "      Batch 44/130\n",
      "      Batch 45/130\n",
      "      Batch 46/130\n",
      "      Batch 47/130\n",
      "      Batch 48/130\n",
      "      Batch 49/130\n",
      "      Batch 50/130\n",
      "      Batch 51/130\n",
      "      Batch 52/130\n",
      "      Batch 53/130\n",
      "      Batch 54/130\n",
      "      Batch 55/130\n",
      "      Batch 56/130\n",
      "      Batch 57/130\n",
      "      Batch 58/130\n",
      "      Batch 59/130\n",
      "      Batch 60/130\n",
      "      Batch 61/130\n",
      "      Batch 62/130\n",
      "      Batch 63/130\n",
      "      Batch 64/130\n",
      "      Batch 65/130\n",
      "      Batch 66/130\n",
      "      Batch 67/130\n",
      "      Batch 68/130\n",
      "      Batch 69/130\n",
      "      Batch 70/130\n",
      "      Batch 71/130\n",
      "      Batch 72/130\n",
      "      Batch 73/130\n",
      "      Batch 74/130\n",
      "      Batch 75/130\n",
      "      Batch 76/130\n",
      "      Batch 77/130\n",
      "      Batch 78/130\n",
      "      Batch 79/130\n",
      "      Batch 80/130\n",
      "      Batch 81/130\n",
      "      Batch 82/130\n",
      "      Batch 83/130\n",
      "      Batch 84/130\n",
      "      Batch 85/130\n",
      "      Batch 86/130\n",
      "      Batch 87/130\n",
      "      Batch 88/130\n",
      "      Batch 89/130\n",
      "      Batch 90/130\n",
      "      Batch 91/130\n",
      "      Batch 92/130\n",
      "      Batch 93/130\n",
      "      Batch 94/130\n",
      "      Batch 95/130\n",
      "      Batch 96/130\n",
      "      Batch 97/130\n",
      "      Batch 98/130\n",
      "      Batch 99/130\n",
      "      Batch 100/130\n",
      "      Batch 101/130\n",
      "      Batch 102/130\n",
      "      Batch 103/130\n",
      "      Batch 104/130\n",
      "      Batch 105/130\n",
      "      Batch 106/130\n",
      "      Batch 107/130\n",
      "      Batch 108/130\n",
      "      Batch 109/130\n",
      "      Batch 110/130\n",
      "      Batch 111/130\n",
      "      Batch 112/130\n",
      "      Batch 113/130\n",
      "      Batch 114/130\n",
      "      Batch 115/130\n",
      "      Batch 116/130\n",
      "      Batch 117/130\n",
      "      Batch 118/130\n",
      "      Batch 119/130\n",
      "      Batch 120/130\n",
      "      Batch 121/130\n",
      "      Batch 122/130\n",
      "      Batch 123/130\n",
      "      Batch 124/130\n",
      "      Batch 125/130\n",
      "      Batch 126/130\n",
      "      Batch 127/130\n",
      "      Batch 128/130\n",
      "      Batch 129/130\n",
      "      Batch 130/130\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 2/3\n",
      "      Batch 1/130\n",
      "      Batch 2/130\n",
      "      Batch 3/130\n",
      "      Batch 4/130\n",
      "      Batch 5/130\n",
      "      Batch 6/130\n",
      "      Batch 7/130\n",
      "      Batch 8/130\n",
      "      Batch 9/130\n",
      "      Batch 10/130\n",
      "      Batch 11/130\n",
      "      Batch 12/130\n",
      "      Batch 13/130\n",
      "      Batch 14/130\n",
      "      Batch 15/130\n",
      "      Batch 16/130\n",
      "      Batch 17/130\n",
      "      Batch 18/130\n",
      "      Batch 19/130\n",
      "      Batch 20/130\n",
      "      Batch 21/130\n",
      "      Batch 22/130\n",
      "      Batch 23/130\n",
      "      Batch 24/130\n",
      "      Batch 25/130\n",
      "      Batch 26/130\n",
      "      Batch 27/130\n",
      "      Batch 28/130\n",
      "      Batch 29/130\n",
      "      Batch 30/130\n",
      "      Batch 31/130\n",
      "      Batch 32/130\n",
      "      Batch 33/130\n",
      "      Batch 34/130\n",
      "      Batch 35/130\n",
      "      Batch 36/130\n",
      "      Batch 37/130\n",
      "      Batch 38/130\n",
      "      Batch 39/130\n",
      "      Batch 40/130\n",
      "      Batch 41/130\n",
      "      Batch 42/130\n",
      "      Batch 43/130\n",
      "      Batch 44/130\n",
      "      Batch 45/130\n",
      "      Batch 46/130\n",
      "      Batch 47/130\n",
      "      Batch 48/130\n",
      "      Batch 49/130\n",
      "      Batch 50/130\n",
      "      Batch 51/130\n",
      "      Batch 52/130\n",
      "      Batch 53/130\n",
      "      Batch 54/130\n",
      "      Batch 55/130\n",
      "      Batch 56/130\n",
      "      Batch 57/130\n",
      "      Batch 58/130\n",
      "      Batch 59/130\n",
      "      Batch 60/130\n",
      "      Batch 61/130\n",
      "      Batch 62/130\n",
      "      Batch 63/130\n",
      "      Batch 64/130\n",
      "      Batch 65/130\n",
      "      Batch 66/130\n",
      "      Batch 67/130\n",
      "      Batch 68/130\n",
      "      Batch 69/130\n",
      "      Batch 70/130\n",
      "      Batch 71/130\n",
      "      Batch 72/130\n",
      "      Batch 73/130\n",
      "      Batch 74/130\n",
      "      Batch 75/130\n",
      "      Batch 76/130\n",
      "      Batch 77/130\n",
      "      Batch 78/130\n",
      "      Batch 79/130\n",
      "      Batch 80/130\n",
      "      Batch 81/130\n",
      "      Batch 82/130\n",
      "      Batch 83/130\n",
      "      Batch 84/130\n",
      "      Batch 85/130\n",
      "      Batch 86/130\n",
      "      Batch 87/130\n",
      "      Batch 88/130\n",
      "      Batch 89/130\n",
      "      Batch 90/130\n",
      "      Batch 91/130\n",
      "      Batch 92/130\n",
      "      Batch 93/130\n",
      "      Batch 94/130\n",
      "      Batch 95/130\n",
      "      Batch 96/130\n",
      "      Batch 97/130\n",
      "      Batch 98/130\n",
      "      Batch 99/130\n",
      "      Batch 100/130\n",
      "      Batch 101/130\n",
      "      Batch 102/130\n",
      "      Batch 103/130\n",
      "      Batch 104/130\n",
      "      Batch 105/130\n",
      "      Batch 106/130\n",
      "      Batch 107/130\n",
      "      Batch 108/130\n",
      "      Batch 109/130\n",
      "      Batch 110/130\n",
      "      Batch 111/130\n",
      "      Batch 112/130\n",
      "      Batch 113/130\n",
      "      Batch 114/130\n",
      "      Batch 115/130\n",
      "      Batch 116/130\n",
      "      Batch 117/130\n",
      "      Batch 118/130\n",
      "      Batch 119/130\n",
      "      Batch 120/130\n",
      "      Batch 121/130\n",
      "      Batch 122/130\n",
      "      Batch 123/130\n",
      "      Batch 124/130\n",
      "      Batch 125/130\n",
      "      Batch 126/130\n",
      "      Batch 127/130\n",
      "      Batch 128/130\n",
      "      Batch 129/130\n",
      "      Batch 130/130\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "    Fold 3/3\n",
      "      Batch 1/130\n",
      "      Batch 2/130\n",
      "      Batch 3/130\n",
      "      Batch 4/130\n",
      "      Batch 5/130\n",
      "      Batch 6/130\n",
      "      Batch 7/130\n",
      "      Batch 8/130\n",
      "      Batch 9/130\n",
      "      Batch 10/130\n",
      "      Batch 11/130\n",
      "      Batch 12/130\n",
      "      Batch 13/130\n",
      "      Batch 14/130\n",
      "      Batch 15/130\n",
      "      Batch 16/130\n",
      "      Batch 17/130\n",
      "      Batch 18/130\n",
      "      Batch 19/130\n",
      "      Batch 20/130\n",
      "      Batch 21/130\n",
      "      Batch 22/130\n",
      "      Batch 23/130\n",
      "      Batch 24/130\n",
      "      Batch 25/130\n",
      "      Batch 26/130\n",
      "      Batch 27/130\n",
      "      Batch 28/130\n",
      "      Batch 29/130\n",
      "      Batch 30/130\n",
      "      Batch 31/130\n",
      "      Batch 32/130\n",
      "      Batch 33/130\n",
      "      Batch 34/130\n",
      "      Batch 35/130\n",
      "      Batch 36/130\n",
      "      Batch 37/130\n",
      "      Batch 38/130\n",
      "      Batch 39/130\n",
      "      Batch 40/130\n",
      "      Batch 41/130\n",
      "      Batch 42/130\n",
      "      Batch 43/130\n",
      "      Batch 44/130\n",
      "      Batch 45/130\n",
      "      Batch 46/130\n",
      "      Batch 47/130\n",
      "      Batch 48/130\n",
      "      Batch 49/130\n",
      "      Batch 50/130\n",
      "      Batch 51/130\n",
      "      Batch 52/130\n",
      "      Batch 53/130\n",
      "      Batch 54/130\n",
      "      Batch 55/130\n",
      "      Batch 56/130\n",
      "      Batch 57/130\n",
      "      Batch 58/130\n",
      "      Batch 59/130\n",
      "      Batch 60/130\n",
      "      Batch 61/130\n",
      "      Batch 62/130\n",
      "      Batch 63/130\n",
      "      Batch 64/130\n",
      "      Batch 65/130\n",
      "      Batch 66/130\n",
      "      Batch 67/130\n",
      "      Batch 68/130\n",
      "      Batch 69/130\n",
      "      Batch 70/130\n",
      "      Batch 71/130\n",
      "      Batch 72/130\n",
      "      Batch 73/130\n",
      "      Batch 74/130\n",
      "      Batch 75/130\n",
      "      Batch 76/130\n",
      "      Batch 77/130\n",
      "      Batch 78/130\n",
      "      Batch 79/130\n",
      "      Batch 80/130\n",
      "      Batch 81/130\n",
      "      Batch 82/130\n",
      "      Batch 83/130\n",
      "      Batch 84/130\n",
      "      Batch 85/130\n",
      "      Batch 86/130\n",
      "      Batch 87/130\n",
      "      Batch 88/130\n",
      "      Batch 89/130\n",
      "      Batch 90/130\n",
      "      Batch 91/130\n",
      "      Batch 92/130\n",
      "      Batch 93/130\n",
      "      Batch 94/130\n",
      "      Batch 95/130\n",
      "      Batch 96/130\n",
      "      Batch 97/130\n",
      "      Batch 98/130\n",
      "      Batch 99/130\n",
      "      Batch 100/130\n",
      "      Batch 101/130\n",
      "      Batch 102/130\n",
      "      Batch 103/130\n",
      "      Batch 104/130\n",
      "      Batch 105/130\n",
      "      Batch 106/130\n",
      "      Batch 107/130\n",
      "      Batch 108/130\n",
      "      Batch 109/130\n",
      "      Batch 110/130\n",
      "      Batch 111/130\n",
      "      Batch 112/130\n",
      "      Batch 113/130\n",
      "      Batch 114/130\n",
      "      Batch 115/130\n",
      "      Batch 116/130\n",
      "      Batch 117/130\n",
      "      Batch 118/130\n",
      "      Batch 119/130\n",
      "      Batch 120/130\n",
      "      Batch 121/130\n",
      "      Batch 122/130\n",
      "      Batch 123/130\n",
      "      Batch 124/130\n",
      "      Batch 125/130\n",
      "      Batch 126/130\n",
      "      Batch 127/130\n",
      "      Batch 128/130\n",
      "      Batch 129/130\n",
      "      Batch 130/130\n",
      "Computing probabilities of each binary class\n",
      "      Computing validation loss...\n",
      "  Regularization alpha = 1.0\n",
      "  Training model...\n",
      "    Fold 1/3\n",
      "      Batch 1/130\n",
      "      Batch 2/130\n",
      "      Batch 3/130\n",
      "      Batch 4/130\n",
      "      Batch 5/130\n",
      "      Batch 6/130\n",
      "      Batch 7/130\n",
      "      Batch 8/130\n",
      "      Batch 9/130\n",
      "      Batch 10/130\n",
      "      Batch 11/130\n",
      "      Batch 12/130\n",
      "      Batch 13/130\n",
      "      Batch 14/130\n",
      "      Batch 15/130\n",
      "      Batch 16/130\n",
      "      Batch 17/130\n",
      "      Batch 18/130\n",
      "      Batch 19/130\n",
      "      Batch 20/130\n",
      "      Batch 21/130\n",
      "      Batch 22/130\n",
      "      Batch 23/130\n",
      "      Batch 24/130\n",
      "      Batch 25/130\n",
      "      Batch 26/130\n",
      "      Batch 27/130\n",
      "      Batch 28/130\n",
      "      Batch 29/130\n",
      "      Batch 30/130\n",
      "      Batch 31/130\n",
      "      Batch 32/130\n",
      "      Batch 33/130\n",
      "      Batch 34/130\n",
      "      Batch 35/130\n"
     ]
    }
   ],
   "source": [
    "print_file_info('./X_data.dat')\n",
    "print_file_info('./y_data.dat')\n",
    "\n",
    "alpha_min,alpha_max,alpha_step = 0.2,2,0.2\n",
    "alphas, errors1 = optimized_training(D=D, teacher_seed=teacher_seed1,alpha_max=alpha_max,alpha_min=alpha_min,alpha_step=alpha_step)\n",
    "if(two_teachers):\n",
    "    generate_data(D=D, teacher_seed=teacher_seed2)\n",
    "    _, errors2 = run_experiment(D=D, teacher_seed=teacher_seed2)\n",
    "    \n",
    "    plot_results(alphas, [errors1, errors2], [f'Teacher 1, seed = {teacher_seed1}', f'Teacher 2, seed = {teacher_seed2}'], D=D, name='combined')\n",
    "    plot_results(alphas, [errors1], [f'Teacher 1, seed = {teacher_seed1}'], D=D, name='teacher 1')\n",
    "    plot_results(alphas, [errors2], [f'Teacher 2, seed = {teacher_seed2}'], D=D, name='teacher 2')\n",
    "else:\n",
    "    plot_results(alphas, [errors1], [f'seed = {teacher_seed1}'], D=D, name='plot')\n",
    "\n",
    "#os.remove('X_data.dat')\n",
    "#os.remove('y_data.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoperon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
