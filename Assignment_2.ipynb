{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: \n",
    "\n",
    "First task for Assignment Part 2 is to code up the teacher model, and use it to generate some data sets of toy sentences for a few different values of the embedding dimensionality ð·(e.g., ð·=100 and ð· = 1000).\n",
    "All the hyperparameter settings are initially to be kept the same as in theÂ Cui et al. paper (the same values were also mentioned in class). But try to write your code in a way where these settings can be easily changed later on, to enable future experiments with the same code.\n",
    "Data generation part at least should be pretty straightforward. After that, the next step should be to get familiar with how to train simple transformer models in PyTorch, so that you can use that to train the student model on the generated data sets. We will discuss the student model further next week.\n",
    "\n",
    "- Use bash for running code?\n",
    "\n",
    "### References:\n",
    "[Cui et al](https://web.iitd.ac.in/~sumeet/Cui_24.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "spinodal = {\n",
    "    'alpha_cross': np.sort([0.5480511155124588,0.7071599112677189,0.9277860822926854,1.2410177405149914,1.06,1.4541109691229464,1.93929723421423,1.7505718608649372,0.8180015676221762]),\n",
    "    'omegas':np.sort([0.,.1,.2,.3,.25,.35,.425,.4,.15]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import colors\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# different colors for attention values\n",
    "c_semantic='deeppink'\n",
    "c_positional='royalblue'\n",
    "c_att = 'rebeccapurple'\n",
    "\n",
    "# different colors for the lienar student\n",
    "c_lin = 'orange'\n",
    "\n",
    "# colors for the phase transition borders\n",
    "c_attlin = 'crimson' \n",
    "c_spinodal = 'forestgreen'\n",
    "\n",
    "# color for changing specific parameters\n",
    "c_no_col = 'black'\n",
    "\n",
    "\n",
    "cmap_uninf = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba(c_semantic)[:3], (1, 1, 1), mcolors.to_rgba(c_positional)[:3]], N=100)\n",
    "cmap_attlin = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba(c_lin)[:3], (1, 1, 1), mcolors.to_rgba(c_att)[:3]], N=100)\n",
    "cmap_pos = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba('#8DE0A8')[:3],\n",
    "                                                    mcolors.to_rgba('#93FFE0')[:3],\n",
    "                                                    mcolors.to_rgba('#85EFFF')[:3],\n",
    "                                                    mcolors.to_rgba('#6BBFFF')[:3],\n",
    "                                                    mcolors.to_rgba(c_positional)[:3]], N=100)\n",
    "cmap_att = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba('#FFE0B6')[:3],\n",
    "                                                    mcolors.to_rgba('#FFB48C')[:3],\n",
    "                                                    mcolors.to_rgba('#FF8166')[:3],\n",
    "                                                    mcolors.to_rgba('#FF3E3B')[:3],\n",
    "                                                    mcolors.to_rgba(c_semantic)[:3]], N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_teacher shape: torch.Size([2200, 2, 1000])\n",
      "T_teacher shape: torch.Size([2200, 2, 1000])\n",
      "W_Q_teacher shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_word_embeddings(L, D, seed=None):\n",
    "    \"\"\"\n",
    "    Generate word embeddings x_l ~ N(0, 0.25 I_D)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate embeddings with std dev = sqrt(0.25) = 0.5\n",
    "    embeddings = torch.normal(mean=0.0, std=0.5, size=(L, D))\n",
    "    return embeddings\n",
    "\n",
    "def positional_component(l, m):\n",
    "    return 0.6 if l == m else 0.4\n",
    "\n",
    "def compute_attention_weights(X, W_Q, omega, DK):\n",
    "    \"\"\"\n",
    "    Compute attention weights of teacher model as a mixture of semantic and positional components\n",
    "    \"\"\"\n",
    "    L, D = X.shape\n",
    "    \n",
    "    # Precompute W_Q^T @ W_Q\n",
    "    WQ_TWQ = W_Q.T @ W_Q  # shape: (D, D)\n",
    "    \n",
    "    # Compute semantic attention scores\n",
    "    semantic_scores = torch.zeros((L, L))\n",
    "    for l in range(L):\n",
    "        for m in range(L):\n",
    "            semantic_scores[l, m] = (X[l] @ WQ_TWQ @ X[m]) / torch.sqrt(torch.tensor(DK))\n",
    "\n",
    "    # Row-wise softmax\n",
    "    softmax_semantic = torch.nn.functional.softmax(semantic_scores, dim=1)\n",
    "\n",
    "    # Positional scores f(l, m)\n",
    "    positional_scores = torch.tensor([[positional_component(l, m) for m in range(L)] for l in range(L)])\n",
    "\n",
    "    # Final mixture\n",
    "    attention_weights = (1 - omega) * softmax_semantic + omega * positional_scores\n",
    "\n",
    "    return attention_weights\n",
    "\n",
    "def generate_single_datapoint(D, L, DK, omega, W_Q, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a single datapoint (X, T)\n",
    "    \"\"\"\n",
    "    X = generate_word_embeddings(L, D, seed)        # (L,D)\n",
    "    A = compute_attention_weights(X, W_Q, omega, DK)# (L,L)\n",
    "    T = torch.matmul(A, X) #(L,D)\n",
    "    return X, T\n",
    "\n",
    "def generate_teacher_dataset(N, D=100, L=2, DK=1, omega=0.3, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a teacher dataset of size N\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate shared query weight matrix\n",
    "    W_Q = torch.normal(mean=0.0, std=1.0, size=(DK,D))\n",
    "    \n",
    "    X_all = torch.zeros((N, L, D))\n",
    "    T_all = torch.zeros((N, L, D))\n",
    "    \n",
    "    for n in range(N):\n",
    "        # Use a different seed for each datapoint\n",
    "        datapoint_seed = seed + n if seed is not None else None\n",
    "        X, T = generate_single_datapoint(D, L, DK, omega, W_Q, datapoint_seed)\n",
    "        X_all[n] = X\n",
    "        T_all[n] = T\n",
    "    \n",
    "    return X_all, T_all, W_Q\n",
    "\n",
    "\n",
    "# parameters\n",
    "L = 2                # Number of words per sentence\n",
    "D = 1000             # Dimensionality\n",
    "N = int(2.2 * D)     # Dataset size\n",
    "DK = 1               # Dimensionality of key/query vectors\n",
    "omega = 0.3          # weight for positional component\n",
    "seed = 42            \n",
    "\n",
    "# Generate teacher dataset\n",
    "X, T, W_Q_teacher = generate_teacher_dataset(\n",
    "    N=N, D=D, L=L, DK=DK, omega=omega, seed=seed\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"X_teacher shape: {X.shape}\") #(N,L,D)\n",
    "print(f\"T_teacher shape: {T.shape}\") #(N,L,D)\n",
    "print(f\"W_Q_teacher shape: {W_Q_teacher.shape}\") #(D_k,D)\n",
    "W=  W_Q_teacher.clone()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Student Model '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class StudentAttentionModel(nn.Module):\n",
    "    def __init__(self, D=100, DK=1, L=2, init_type='random', W_Q_teacher=None):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.DK = DK\n",
    "        self.L = L\n",
    "\n",
    "        # Positional encoding: r1 = -r2 = 1_D\n",
    "        r = torch.ones(D)\n",
    "        self.register_buffer('R', torch.stack([r, -r], dim=0))  # (L, D)\n",
    "\n",
    "        self.W_Q = nn.Parameter(torch.empty(DK, D))\n",
    "\n",
    "        # initialization of W\n",
    "        if init_type == 'semantic':\n",
    "            self.W_Q = torch.nn.Parameter(W_Q_teacher.reshape(-1, 1))\n",
    "        elif init_type == 'positional':\n",
    "            self.W_Q = torch.nn.Parameter(torch.ones(D).reshape(-1, 1))\n",
    "        else:\n",
    "            self.W_Q = torch.nn.Parameter(0.1*torch.randn(D, DK))\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch_size, L, D)\n",
    "        Returns:\n",
    "            Y: (batch_size, L, D)\n",
    "        \"\"\"\n",
    "        # Add positional encodings\n",
    "        X_pos = X + self.R.unsqueeze(0)  # (B, L, D)\n",
    "        \n",
    "        xQ = torch.einsum(\"imk,kl->iml\", X_pos, self.W_Q/np.sqrt(self.D)) # why have they divided by D\n",
    "        A = torch.nn.Softmax(dim=-1)(torch.einsum(\"iml,inl->imn\", xQ, xQ))\n",
    "        Y = torch.einsum(\"imn,inj->imj\", A, X_pos)\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "def loss_SSE(Y, T):\n",
    "    loss = F.mse_loss(Y, T, reduction='sum') / (2 * X.shape[2])  # SSE\n",
    "    return loss\n",
    "# change num epochs\n",
    "\n",
    "def train_student(X_train, T_train, X_test, T_test, lam=1e-2, lr=0.15, epochs=5,init_type='semantic',W_Q_teacher = None,DK=1,batch_size = D):\n",
    "    '''\n",
    "    Trains a student attention model with specified initialization. \n",
    "    lam : L2 regularisation parameter\n",
    "    '''\n",
    "\n",
    "    # Create a DataLoader for mini-batching\n",
    "    train_dataset = TensorDataset(X_train, T_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    N, L, D = X_train.shape\n",
    "    init_type= None\n",
    "    model = StudentAttentionModel(\n",
    "        D=D, DK=DK, L=L,\n",
    "        init_type=init_type,\n",
    "        W_Q_teacher=W_Q_teacher\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.SGD([{'params': [model.W_Q],\"weight_decay\":lam }], lr=0.15)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, T_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_SSE(model(X_batch), T_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    gen_error = loss_SSE(model(X_test),T_test).item()/T_test.shape[0]\n",
    "    W_Q_flat = model.W_Q.flatten()\n",
    "    train_error = loss.item()+lam/2*float(torch.sum(W_Q_flat**2))\n",
    "    r1 = torch.ones(D)\n",
    "    m = np.abs(float(r1@ W_Q_flat /D))\n",
    "    theta = np.abs(float(W_Q_teacher @ W_Q_flat/D))\n",
    "\n",
    "    return model,gen_error,train_error,m,theta \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "-   Do CV for lambda\n",
    "-   train for both inits\n",
    "-   Reproduce Fig. 2: (part a done)\n",
    "-       alpha (0,2) adjust \n",
    "-       look at both training & test error (clear the loss vs MSE confusion)\n",
    "-       alpha vs delta error\n",
    "-       alpha vs summary stats (compute them with modular fns)\n",
    "-       dense linear model\n",
    "-   Reproduce Fig. 3:\n",
    "-       summary stat concentration plot\n",
    "-       color map of delta e wrt omega and alpha change\n",
    "-       attention model - linear baseline wrt omega and alpha change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Alpha = 0.20 | N_train = 200 ===\n",
      "[Semantic Init]\n",
      "[Positional Init]\n",
      "sem 1.0815787506103516 pos 1.0815787506103516\n",
      "delta 0.0\n",
      "\n",
      "=== Alpha = 0.40 | N_train = 400 ===\n",
      "[Semantic Init]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    103\u001b[39m     plt.xlim(\u001b[32m0.0\u001b[39m,\u001b[32m2.0\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m#plt.plot(df_semantic[df_semantic[\"alpha\"]>=1.24].alpha,df_semantic[df_semantic[\"alpha\"]>=1.24].theta/sigma**2, color=c_semantic)\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m#plt.plot(df_positional[df_positional[\"alpha\"]>=1.24].alpha,df_positional[df_positional[\"alpha\"]>=1.24].theta/sigma**2, color=c_positional)\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m#plt.scatter(dfi[dfi[\"alpha\"]>=1.24].alpha,dfi[dfi[\"alpha\"]>=1.24].attention_theta_mean,label=r'$\\theta/\\sigma^2$', color=c_semantic)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m \u001b[38;5;66;03m#plt.scatter(dfu[dfu[\"alpha\"]<=1.24].alpha,dfu[dfu[\"alpha\"]<=1.24].attention_mag_mean/sigma**2, color=c_positional)\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m#plt.scatter(dfi[dfi[\"alpha\"]>=1.24].alpha,dfi[dfi[\"alpha\"]>=1.24].attention_mag_mean/sigma**2,label=r'$m/\\sigma^2$', color=c_positional)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m results = \u001b[43mrun_fig_2A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW_Q_teacher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mW_Q_teacher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m plot_fig_2A(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrun_fig_2A\u001b[39m\u001b[34m(X, T, lam, lr, epochs, DK, W_Q_teacher)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Semantic Init\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[Semantic Init]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m model_sem, e_gen_sem,e_train_sem,m_sem,theta_sem = \u001b[43mtrain_student\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m                          \u001b[49m\u001b[43minit_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msemantic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_Q_teacher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mW_Q_teacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Positional Init\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[Positional Init]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain_student\u001b[39m\u001b[34m(X_train, T_train, X_test, T_test, lam, lr, epochs, init_type, W_Q_teacher, DK, batch_size)\u001b[39m\n\u001b[32m     70\u001b[39m         optimizer.zero_grad()\n\u001b[32m     71\u001b[39m         loss = loss_SSE(model(X_batch), T_batch)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m         optimizer.step()\n\u001b[32m     74\u001b[39m gen_error = loss_SSE(model(X_test),T_test).item()/T_test.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Internship/anaconda3/envs/pyoperon/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    517\u001b[39m         Tensor.backward,\n\u001b[32m    518\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m         inputs=inputs,\n\u001b[32m    524\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Internship/anaconda3/envs/pyoperon/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    262\u001b[39m     retain_graph = create_graph\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Internship/anaconda3/envs/pyoperon/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def run_fig_2A(X, T, lam=1e-3, lr=0.15, epochs=500, DK=1,W_Q_teacher = None):\n",
    "    \"\"\"\n",
    "    Runs the generalization experiment comparing semantic vs positional initialization\n",
    "    across sample complexity Î± = N_train / D.\n",
    "    \n",
    "    Args:\n",
    "        X, T: torch.Tensor of shape (N, L, D)\n",
    "        lam: regularization strength\n",
    "        lr: learning rate\n",
    "        epochs: number of training epochs\n",
    "        DK: key/query dimension\n",
    "\n",
    "    Returns:\n",
    "        results: list of dicts with alpha, semantic_loss, positional_loss, and delta\n",
    "    \"\"\"\n",
    "    D = X.shape[2]\n",
    "    L = X.shape[1]\n",
    "    alphas = np.linspace(0.2, 2.0, 10)\n",
    "    results = []\n",
    "\n",
    "    # Fixed test set\n",
    "    N_test = int(0.2 * D)\n",
    "    X_test = X[-N_test:]\n",
    "    T_test = T[-N_test:]\n",
    "\n",
    "    for alpha in alphas:\n",
    "        N_train = int(alpha * D)\n",
    "        X_train = X[:N_train]\n",
    "        T_train = T[:N_train]\n",
    "\n",
    "        print(f\"\\n=== Alpha = {alpha:.2f} | N_train = {N_train} ===\")\n",
    "\n",
    "        # Semantic Init\n",
    "        print(\"[Semantic Init]\")\n",
    "        model_sem, e_gen_sem,e_train_sem,m_sem,theta_sem = train_student(X_train, T_train, X_test, T_test, lam, lr, epochs,\n",
    "                                  init_type='semantic', W_Q_teacher=W_Q_teacher, DK=DK)\n",
    "\n",
    "        # Positional Init\n",
    "        print(\"[Positional Init]\")\n",
    "        model_pos, e_gen_pos,e_train_pos,m_pos,theta_pos = train_student(X_train, T_train, X_test, T_test, lam, lr, epochs,\n",
    "                                  init_type='positional', DK=DK,W_Q_teacher=W_Q_teacher)\n",
    "\n",
    "        # Compute losses\n",
    "\n",
    "        delta_gen = e_gen_pos - e_gen_sem\n",
    "\n",
    "        delta_train = e_train_pos- e_train_sem\n",
    "\n",
    "        print(f'sem {e_gen_sem} pos {e_gen_pos}')\n",
    "        print(f'delta {delta_gen}')\n",
    "\n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'semantic_loss_gen':e_gen_sem ,\n",
    "            'positional_loss_gen': e_gen_pos,\n",
    "            'semantic_loss_train': e_train_sem,\n",
    "            'positional_loss_train': e_train_pos,\n",
    "            'delta_gen': delta_gen,\n",
    "            'delta_train': delta_train,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_fig_2A(results):\n",
    "    \"\"\"\n",
    "    Plots delta (positional - semantic) loss vs alpha.\n",
    "    \"\"\"\n",
    "    alphas = [r['alpha'] for r in results]\n",
    "    deltas_gen = [r['delta_gen'] for r in results]\n",
    "    deltas_train = [r['delta_train'] for r in results]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 3.8))\n",
    "    plt.plot(alphas, deltas_gen, c='grey', label='Generalization')\n",
    "    #plt.plot(alphas, deltas_train, c='black', linestyle='--', label='Training')\n",
    "    plt.axhline(0, linestyle='--', color='grey')\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.ylabel(r'$\\Delta \\epsilon_t$')\n",
    "    plt.xlim(0.0, 2.0)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_fig_2B(results):\n",
    "    \"\"\"\n",
    "    Plots summary statistics(theta,m) vs alpha.\n",
    "    \"\"\"\n",
    "    alphas = [r['alpha'] for r in results]\n",
    "    deltas_gen = [r['delta_gen'] for r in results]\n",
    "    deltas_train = [r['delta_train'] for r in results]\n",
    "\n",
    "   \n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.0,2.0)\n",
    "\n",
    "\n",
    "\n",
    "#plt.plot(df_semantic[df_semantic[\"alpha\"]>=1.24].alpha,df_semantic[df_semantic[\"alpha\"]>=1.24].theta/sigma**2, color=c_semantic)\n",
    "#plt.plot(df_positional[df_positional[\"alpha\"]>=1.24].alpha,df_positional[df_positional[\"alpha\"]>=1.24].theta/sigma**2, color=c_positional)\n",
    "#plt.scatter(dfi[dfi[\"alpha\"]>=1.24].alpha,dfi[dfi[\"alpha\"]>=1.24].attention_theta_mean,label=r'$\\theta/\\sigma^2$', color=c_semantic)\n",
    "#plt.scatter(dfu[dfu[\"alpha\"]<=1.24].alpha,dfu[dfu[\"alpha\"]<=1.24].attention_theta_mean, color=c_semantic)\n",
    "\n",
    "#plt.plot(df_semantic[df_semantic[\"alpha\"]<=1.24].alpha,df_semantic[df_semantic[\"alpha\"]<=1.24].m/sigma**2,  color=c_semantic)\n",
    "#plt.plot(df_positional[df_positional[\"alpha\"]<=1.24].alpha,df_positional[df_positional[\"alpha\"]<=1.24].m/sigma**2,color=c_positional)\n",
    "#plt.scatter(dfu[dfu[\"alpha\"]<=1.24].alpha,dfu[dfu[\"alpha\"]<=1.24].attention_mag_mean/sigma**2, color=c_positional)\n",
    "#plt.scatter(dfi[dfi[\"alpha\"]>=1.24].alpha,dfi[dfi[\"alpha\"]>=1.24].attention_mag_mean/sigma**2,label=r'$m/\\sigma^2$', color=c_positional)\n",
    "\n",
    "results = run_fig_2A(X, T,W_Q_teacher=W_Q_teacher)\n",
    "plot_fig_2A(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoperon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
