{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: \n",
    "\n",
    "First task for Assignment Part 2 is to code up the teacher model, and use it to generate some data sets of toy sentences for a few different values of the embedding dimensionality ð·(e.g., ð·=100 and ð· = 1000).\n",
    "All the hyperparameter settings are initially to be kept the same as in theÂ Cui et al. paper (the same values were also mentioned in class). But try to write your code in a way where these settings can be easily changed later on, to enable future experiments with the same code.\n",
    "Data generation part at least should be pretty straightforward. After that, the next step should be to get familiar with how to train simple transformer models in PyTorch, so that you can use that to train the student model on the generated data sets. We will discuss the student model further next week.\n",
    "\n",
    "- Use bash for running code?\n",
    "\n",
    "### References:\n",
    "[Cui et al](https://web.iitd.ac.in/~sumeet/Cui_24.pdf)\n",
    "\n",
    "# TODO:\n",
    "-   Do CV for lambda\n",
    "-   implement plots for both (train,gen)\n",
    "-   rest is just plots\n",
    "-   fig 2A,2B,3B code all entangled ()\n",
    "-   fig 3C,2C code entangled\n",
    "-   fig 3A completely independent\n",
    "-   the business of taking means,multiple iterations etc...\n",
    "\n",
    "# Checks:\n",
    "-   Teacher model is exactly same as theirs\n",
    "-   Student model :\n",
    "        - D vs D_k norm confusion\n",
    "        - pos and sem are same for D_k norm\n",
    "        - shapes are weird -> causing problems in m,theta\n",
    "\n",
    "# Do final runs:\n",
    "### Run 1:\n",
    "- Use sqrt(D)\n",
    "- Use CV lamba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import colors\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# different colors for attention values\n",
    "c_semantic='deeppink'\n",
    "c_positional='royalblue'\n",
    "c_att = 'rebeccapurple'\n",
    "\n",
    "# different colors for the lienar student\n",
    "c_lin = 'orange'\n",
    "\n",
    "# colors for the phase transition borders\n",
    "c_attlin = 'crimson' \n",
    "c_spinodal = 'forestgreen'\n",
    "\n",
    "# color for changing specific parameters\n",
    "c_no_col = 'black'\n",
    "\n",
    "\n",
    "cmap_uninf = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba(c_semantic)[:3], (1, 1, 1), mcolors.to_rgba(c_positional)[:3]], N=100)\n",
    "cmap_attlin = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba(c_lin)[:3], (1, 1, 1), mcolors.to_rgba(c_att)[:3]], N=100)\n",
    "cmap_pos = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba('#8DE0A8')[:3],\n",
    "                                                    mcolors.to_rgba('#93FFE0')[:3],\n",
    "                                                    mcolors.to_rgba('#85EFFF')[:3],\n",
    "                                                    mcolors.to_rgba('#6BBFFF')[:3],\n",
    "                                                    mcolors.to_rgba(c_positional)[:3]], N=100)\n",
    "cmap_att = LinearSegmentedColormap.from_list('INF-UNINF',\n",
    "                                                   [mcolors.to_rgba('#FFE0B6')[:3],\n",
    "                                                    mcolors.to_rgba('#FFB48C')[:3],\n",
    "                                                    mcolors.to_rgba('#FF8166')[:3],\n",
    "                                                    mcolors.to_rgba('#FF3E3B')[:3],\n",
    "                                                    mcolors.to_rgba(c_semantic)[:3]], N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_teacher shape: torch.Size([2200, 2, 1000])\n",
      "T_teacher shape: torch.Size([2200, 2, 1000])\n",
      "W_Q_teacher shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# they divided by sqrt(D) should you do that ?\n",
    "def generate_teacher_dataset(N, D=100, L=2, DK=1, omega=0.3, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a teacher dataset of size N\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate shared query weight matrix\n",
    "    W_Q = torch.randn(D)\n",
    "    sigma = 0.5 # sample X from multivariate gaussian, var->0.25\n",
    "    X_all = sigma * torch.randn((N, L, D)) \n",
    "\n",
    "    softmax=torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    xQ = torch.einsum(\"imk,k->im\", X_all, W_Q)\n",
    "    att_score = softmax(torch.einsum(\"im,in->imn\", xQ, xQ))\n",
    "\n",
    "    pos_matrix = torch.Tensor(np.array([[.6,.4],[.4,.6]]))\n",
    "\n",
    "    T_all = (1-omega) * torch.einsum(\"imn,inj->imj\", att_score, X_all) + omega * torch.einsum(\"nld,fl->nfd\", X_all, pos_matrix)\n",
    "\n",
    "    return X_all, T_all, W_Q\n",
    "\n",
    "\n",
    "# parameters\n",
    "L = 2                # Number of words per sentence\n",
    "D = 1000           # Dimensionality\n",
    "N = int(2.2 * D)     # Dataset size\n",
    "DK = 1               # Dimensionality of key/query vectors\n",
    "omega = 0.3          # weight for positional component\n",
    "seed = 42            \n",
    "sigma = 0.5\n",
    "# Generate teacher dataset\n",
    "X, T, W_Q_teacher = generate_teacher_dataset(\n",
    "    N=N, D=D, L=L, DK=DK, omega=omega, seed=seed\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"X_teacher shape: {X.shape}\") #(N,L,D)\n",
    "print(f\"T_teacher shape: {T.shape}\") #(N,L,D)\n",
    "print(f\"W_Q_teacher shape: {W_Q_teacher.shape}\") #(D_k,D)\n",
    "W =  W_Q_teacher.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Student Model '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# here they divide positional encoding by sqrt(D) why investigate (same in forward and W_Q)\n",
    "# check the positional embeddings once\n",
    "class StudentAttentionModel(nn.Module):\n",
    "    def __init__(self, D=100, DK=1, L=2, init_type='random', WQ_teacher=None):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.DK = DK\n",
    "        self.L = L\n",
    "\n",
    "        # initialization of W\n",
    "        if init_type == 'semantic':\n",
    "            self.W_Q = torch.nn.Parameter(WQ_teacher.reshape(-1, 1))\n",
    "        elif init_type == 'positional':\n",
    "\n",
    "            self.W_Q = torch.nn.Parameter(torch.ones(D).reshape(-1, 1))\n",
    "        else:\n",
    "            self.W_Q = torch.nn.Parameter(torch.randn(D, DK))\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch_size, L, D)\n",
    "        Returns:\n",
    "            Y: (batch_size, L, D)\n",
    "        \"\"\"\n",
    "        # Add positional encodings\n",
    "        B,L,D = X.shape\n",
    "        # Positional encoding: r1 = -r2 = 1_D\n",
    "        r1= torch.ones(D)\n",
    "        R = torch.vstack((r1, -r1))\n",
    "        Rs =  np.repeat(R[np.newaxis, :, :], B, axis=0)\n",
    "\n",
    "        X_pos = X + Rs  # (B, L, D)\n",
    "        \n",
    "        xQ = torch.einsum(\"imk,kl->iml\", X_pos, self.W_Q) # why have they divided by D\n",
    "        A = torch.nn.Softmax(dim=-1)(torch.einsum(\"iml,inl->imn\", xQ, xQ))\n",
    "        Y = torch.einsum(\"imn,inj->imj\", A, X_pos)\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "def loss_SSE(Y, T):\n",
    "    return torch.sum((Y-T)**2)/2/T.shape[-1]\n",
    "# change num epochs\n",
    "\n",
    "def train_student(X_train, T_train, X_test, T_test, lam=1e-2, lr=0.15, epochs=5000,init_type='semantic',W_Q_teacher = None,DK=1):\n",
    "    '''\n",
    "    Trains a student attention model with specified initialization. \n",
    "    lam : L2 regularisation parameter\n",
    "    '''\n",
    "    N, L, D = X_train.shape\n",
    "    \n",
    "    # Create a DataLoader for mini-batching\n",
    "    train_dataset = TensorDataset(X_train, T_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=N, shuffle=True)\n",
    "\n",
    "\n",
    "    model = StudentAttentionModel(\n",
    "        D=D, DK=DK, L=L,\n",
    "        init_type=init_type,\n",
    "        WQ_teacher=W_Q_teacher\n",
    "    )\n",
    "\n",
    "    gen_error_list = []\n",
    "    train_error_list = []\n",
    "    m_list,theta_list = [],[]\n",
    "    r1 = torch.ones(D)\n",
    "\n",
    "    optimizer = torch.optim.SGD([{'params': [model.W_Q],\"weight_decay\":lam }], lr=0.15)\n",
    "\n",
    "    #print(model.W_Q[:2],init_type)\n",
    "    er_list = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for X_batch, T_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(X_batch)\n",
    "            loss = loss_SSE(Y_pred, T_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        W_Q_flat = model.W_Q.flatten()\n",
    "        W_Q_teacher_flat = W_Q_teacher.flatten()\n",
    "        if(epoch > epochs-20):\n",
    "            gen_error_list.append(loss_SSE(model(X_test),T_test).item()/T_test.shape[0])\n",
    "            train_error = (loss.item()/T_train.shape[0]) +lam/2*float(torch.sum(W_Q_flat**2))\n",
    "            train_error_list.append(train_error)\n",
    "            m = np.abs(float(r1@ W_Q_flat /D))\n",
    "            theta = np.abs(float(W_Q_teacher_flat @ W_Q_flat/D))\n",
    "            m_list.append(m)\n",
    "            theta_list.append(theta)\n",
    "        er_list.append(epoch_loss/num_batches)\n",
    "    plt.figure()\n",
    "    plt.plot(er_list)\n",
    "    plt.show()\n",
    " \n",
    "    return model, np.mean(gen_error_list[-5:]),np.mean(train_error_list[-5:]), np.mean(m_list[-5:]), np.mean(theta_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01       0.19090909 0.37181818 0.55272727 0.73363636 0.91454545\n",
      " 1.09545455 1.27636364 1.45727273 1.63818182 1.81909091 2.        ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def train_student_L2_CV(X_train, T_train, X_val, T_val, lam_list, lr=0.15, epochs=5000, init_type='semantic', W_Q_teacher=None, DK=1):\n",
    "    '''\n",
    "    Performs cross-validation over a list of lambda values and returns the best lambda and associated metrics.\n",
    "    '''\n",
    "    results = []\n",
    "\n",
    "    for lam in lam_list:\n",
    "        model, gen_error, train_error, m, theta = train_student(\n",
    "            X_train, T_train,\n",
    "            X_val, T_val,\n",
    "            lam=lam,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            init_type=init_type,\n",
    "            W_Q_teacher=W_Q_teacher,\n",
    "            DK=DK\n",
    "        )\n",
    "        results.append({\n",
    "            'lam': lam,\n",
    "            'gen_error': gen_error,\n",
    "            'train_error': train_error,\n",
    "            'm': m,\n",
    "            'theta': theta,\n",
    "            'model': model\n",
    "        })\n",
    "\n",
    "    # Find best lambda (lowest generalization error)\n",
    "    best_result = min(results, key=lambda x: x['gen_error'])\n",
    "    \n",
    "    return best_result['model'],best_result['gen_error'],best_result['train_error'],best_result['m'],best_result['theta']\n",
    "\n",
    "\n",
    "def run_fig_2A(X, T, lam=[1e-2], lr=0.15, epochs=5000, DK=1,WQ_teacher = None,alphas=None):\n",
    "    \"\"\"\n",
    "    Runs the generalization experiment comparing semantic vs positional initialization\n",
    "    across sample complexity Î± = N_train / D.\n",
    "    \n",
    "    Args:\n",
    "        X, T: torch.Tensor of shape (N, L, D)\n",
    "        lam: regularization strength\n",
    "        lr: learning rate\n",
    "        epochs: number of training epochs\n",
    "        DK: key/query dimension\n",
    "\n",
    "    Returns:\n",
    "        results: list of dicts with alpha, semantic_loss, positional_loss, and delta\n",
    "    \"\"\"\n",
    "\n",
    "    D = X.shape[2]\n",
    "    L = X.shape[1]\n",
    "    results = []\n",
    "\n",
    "    # Fixed test set\n",
    "    N_test = int(0.2 * D)\n",
    "    X_test = X[-N_test:]\n",
    "    T_test = T[-N_test:]\n",
    "\n",
    "    for alpha in alphas:\n",
    "        N_train = int(alpha * D)\n",
    "        X_train = X[:N_train]\n",
    "        T_train = T[:N_train]\n",
    "\n",
    "        print(f\"\\n=== Alpha = {alpha:.2f} | N_train = {N_train} ===\")\n",
    "\n",
    "        W = W_Q_teacher.clone().detach()\n",
    "        # Semantic Init\n",
    "        #print(\"[Semantic Init]\")\n",
    "        model_sem, e_gen_sem,e_train_sem,m_sem,theta_sem = train_student_L2_CV(X_train, T_train, X_test, T_test, lam, lr, epochs,\n",
    "                                  init_type='semantic', W_Q_teacher=W, DK=DK)\n",
    "        W = W_Q_teacher.clone().detach()\n",
    "        # Positional Init\n",
    "        #print(\"[Positional Init]\")\n",
    "        model_pos, e_gen_pos,e_train_pos,m_pos,theta_pos = train_student_L2_CV(X_train, T_train, X_test, T_test, lam, lr, epochs,\n",
    "                                  init_type='positional', DK=DK,W_Q_teacher=W)\n",
    "\n",
    "        # Compute losses\n",
    "\n",
    "        delta_gen =e_gen_pos-e_gen_sem\n",
    "\n",
    "        delta_train =  e_train_pos-e_train_sem\n",
    "\n",
    "        print('delta_train',delta_train)\n",
    "\n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'semantic_loss_gen':e_gen_sem ,\n",
    "            'positional_loss_gen': e_gen_pos,\n",
    "            'semantic_loss_train': e_train_sem,\n",
    "            'positional_loss_train': e_train_pos,\n",
    "            'delta_gen': delta_gen,\n",
    "            'delta_train': delta_train,\n",
    "            'theta_sem':theta_sem,\n",
    "            'theta_pos':theta_pos,\n",
    "            'm_pos':m_pos,\n",
    "            'm_sem':m_sem \n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "instances = 1\n",
    "results = []\n",
    "\n",
    "def run_fig_2A_mean(D,omega,L, lam=1e-2, lr=0.15, epochs=5000, DK=1, WQ_teacher=None, alphas=None, instances=24):\n",
    "    mean_results = []\n",
    "    N = int((max(alphas)+0.2)*D)  \n",
    "    for inst in range(instances):\n",
    "        print(f\"Running iteration {inst + 1}/{instances}\")\n",
    "        X, T, W_Q_teacher = generate_teacher_dataset(N=N, D=D, L=L, DK=DK, omega=omega)\n",
    "\n",
    "        results = run_fig_2A(X, T, lam=lam, lr=lr, epochs=epochs, DK=DK, WQ_teacher=W_Q_teacher, alphas=alphas)\n",
    "        \n",
    "        if not mean_results:\n",
    "            for result in results:\n",
    "                mean_results.append({\n",
    "                    key: [result[key]] for key in result\n",
    "                })\n",
    "        else:\n",
    "            for i, result in enumerate(results):\n",
    "                for key in result:\n",
    "                    mean_results[i][key].append(result[key])\n",
    "\n",
    "    for i, result in enumerate(mean_results):\n",
    "        for key in result:\n",
    "            \n",
    "            result[key] = np.mean(result[key])\n",
    "            if(key in ['delta_train']):\n",
    "                print(f'{result[key]} {key}')\n",
    "\n",
    "    return mean_results\n",
    "lr = 0.15 #0.15 # 0.15\n",
    "alphas = np.linspace(0.01,2,12)\n",
    "print(alphas)\n",
    "lambdas_grid = np.logspace(-2.5,-1.5,3)\n",
    "#results = run_fig_2A_mean(D,omega,L,lr=lr,lam=lambdas_grid,epochs =1,alphas=alphas,instances=instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x380 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x570 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_fig_2A(results):\n",
    "\n",
    "    alphas = [r['alpha'] for r in results]\n",
    "    deltas_gen = np.array([r['delta_gen'] for r in results])\n",
    "    deltas_train = [r['delta_train'] for r in results]\n",
    "    print(deltas_train)\n",
    "\n",
    "    #fig = plt.figure(figsize=(4, 3.8))\n",
    "    fig = plt.figure(figsize=(4, 3.8))\n",
    "    plt.plot(alphas,deltas_gen, c='grey', label='Generalization')\n",
    "    plt.scatter(alphas,deltas_gen, c='grey', label='Generalization')\n",
    "   # plt.plot(alphas, deltas_train, c='black', linestyle='--', label='Training')\n",
    "    plt.axhline(0, linestyle='--', color='grey')\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.ylabel(r'$\\Delta \\epsilon_t$')\n",
    "    plt.xlim(0.0, 2.0)\n",
    "    #plt.ylim(-1e-1,1e-1)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_fig_2B(results):\n",
    "    \"\"\"\n",
    "    Plots summary statistics(theta,m) vs alpha.\n",
    "    \"\"\"\n",
    "    alphas = [r['alpha'] for r in results]\n",
    "    m_pos = [r['m_pos'] for r in results]\n",
    "    m_sem = [r['m_sem'] for r in results]\n",
    "    theta_pos = [r['theta_pos'] for r in results]\n",
    "    theta_sem = [r['theta_sem'] for r in results]\n",
    "    theta_pos,theta_sem = np.array(theta_pos),np.array(theta_sem)\n",
    "    m_pos,m_sem = np.array(m_pos),np.array(m_sem)\n",
    "    fig=plt.figure(figsize=(4*1.5,3.8*1.5))\n",
    "\n",
    "    sigma=0.5 # variance 0.25\n",
    "    \n",
    "    plt.plot(alphas,theta_sem/sigma**2, color=c_semantic)\n",
    "    plt.plot(alphas,theta_pos/sigma**2, color=c_semantic)\n",
    "\n",
    "    plt.scatter(alphas,theta_pos/sigma**2,label=r'$\\theta/\\sigma^2 pos$', color=c_semantic)\n",
    "    plt.scatter(alphas,theta_sem/sigma**2,label=r'$\\theta/\\sigma^2 sem$', color=c_semantic)\n",
    "\n",
    "    plt.plot(alphas,m_sem/sigma**2,  color=c_positional)\n",
    "    plt.plot(alphas,m_pos/sigma**2,color=c_positional)\n",
    "    plt.scatter(alphas,m_pos/sigma**2,label=r'$m/\\sigma^2 pos$', color=c_positional)\n",
    "    plt.scatter(alphas,m_sem/sigma**2,label=r'$m/\\sigma^2 sem$', color=c_positional)\n",
    "    \n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.0,2.0)\n",
    "\n",
    "plot_fig_2A(results)\n",
    "plot_fig_2B(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "/var/folders/t2/b95676ns3dsgtbbt3q5ckmk40000gn/T/ipykernel_84349/1011834393.py:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  '''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.4\n",
      "0.6000000000000001\n",
      "0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t2/b95676ns3dsgtbbt3q5ckmk40000gn/T/ipykernel_84349/1011834393.py:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[33;03m    lin_error = results[]\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    plt.plot(df_positional[df_positional[\"alpha\"]<1.25].alpha,df_positional[df_positional[\"alpha\"]<1.25].mse/2, color=c_neutral,label=r'$\\epsilon_g^{\\mathrm{minloss}}$')\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \u001b[33;03m    plt.show()\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m    112\u001b[39m alphas = np.linspace(\u001b[32m0.2\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m10\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m results2 = \u001b[43mrun_fig_2C\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43malphas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m plot_fig2C(results,results2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_fig_2C\u001b[39m\u001b[34m(X, T, alphas)\u001b[39m\n\u001b[32m     48\u001b[39m     X_train = X[:N_train]\n\u001b[32m     49\u001b[39m     T_train = T[:N_train]\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     elin, elin_train = \u001b[43mtrain_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlmbda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m          \n\u001b[32m     53\u001b[39m     results.append({\n\u001b[32m     54\u001b[39m         \u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m: alpha,\n\u001b[32m     55\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlinear_gen_error\u001b[39m\u001b[33m'\u001b[39m: elin,\n\u001b[32m     56\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlinear_train_error\u001b[39m\u001b[33m'\u001b[39m:elin_train\n\u001b[32m     57\u001b[39m     })\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_linear\u001b[39m\u001b[34m(X_train, T_train, X_test, y_test, lmbda)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m         loss = loss_SSE(y_pred, y)\n\u001b[32m     26\u001b[39m         optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Internship/anaconda3/envs/pyoperon/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Internship/anaconda3/envs/pyoperon/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class LinearModel(torch.nn.Module):  # linear attention\n",
    "    def __init__(self, L):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.B = torch.nn.Parameter(torch.randn(L, L))\n",
    "\n",
    "    def forward(self, x):\n",
    "        yhat = self.B@x\n",
    "        return yhat\n",
    "\n",
    "# use GD\n",
    "def train_linear(X_train,T_train, X_test, y_test,lmbda):\n",
    "\n",
    "    model = LinearModel(L)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{'params': [model.B], \"weight_decay\":lmbda}], lr=0.15)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, T_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=D, shuffle=True)\n",
    "\n",
    "    n_iter = 200\n",
    "    for t in range(n_iter):\n",
    "        for x, y in train_loader:\n",
    "            y_pred = model(x)\n",
    "            loss = loss_SSE(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    gen_loss = loss_SSE(model(X_test),y_test).item()/y_test.shape[0]\n",
    "    train_loss = loss.item()+lmbda/2*float(torch.sum(model.B.cpu().flatten()**2))\n",
    "\n",
    "    return gen_loss,train_loss\n",
    "\n",
    "def run_fig_2C(X,T,alphas):\n",
    "    D = X.shape[2]\n",
    "    L = X.shape[1]\n",
    "    results = []\n",
    "\n",
    "    # Fixed test set\n",
    "    N_test = int(0.2 * D)\n",
    "    X_test = X[-N_test:]\n",
    "    T_test = T[-N_test:]\n",
    "\n",
    "    for alpha in alphas:\n",
    "        print(alpha)\n",
    "        N_train = int(alpha * D)\n",
    "        X_train = X[:N_train]\n",
    "        T_train = T[:N_train]\n",
    "\n",
    "        elin, elin_train = train_linear(X_train,T_train, X_test, T_test,lmbda=1e-4)          \n",
    "\n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'linear_gen_error': elin,\n",
    "            'linear_train_error':elin_train\n",
    "        })\n",
    "\n",
    "    return (results)\n",
    "\n",
    "def plot_fig2C(results_1,results_2):\n",
    "    '''\n",
    "    Plots for the dense linear baseline\n",
    "    '''\n",
    "    fig=plt.figure(figsize=(4,3.8))\n",
    "\n",
    "    c_neutral = c_att  # 'black'\n",
    "\n",
    "    alphas = np.array([result['alpha'] for result in results_2])\n",
    "    lin_err = [result['linear_gen_error'] for result in results_2]\n",
    "    sem_gen_err = np.array([r['semantic_loss_gen'] for r in results_1])\n",
    "    pos_gen_err = np.array([r['positional_loss_gen'] for r in results_1])\n",
    "    mask = alphas < 41.25\n",
    "\n",
    "    plt.plot(alphas,lin_err)\n",
    "    plt.axhline(np.mean(lin_err), color=c_lin, label=r'$\\epsilon_g^{\\mathrm{lin}}$')#,ls=\"--\")\n",
    "    #plt.scatter(alphas[mask],pos_gen_err[mask],color=c_neutral)\n",
    "    #plt.plot(alphas[mask],pos_gen_err[mask],color=c_neutral)\n",
    "    mask = ~mask\n",
    "    plt.scatter(alphas[~mask],sem_gen_err[~mask],color=c_neutral)\n",
    "    plt.plot(alphas[~mask],sem_gen_err[~mask],color=c_neutral)\n",
    "    plt.scatter(alphas[~mask],pos_gen_err[~mask],color=c_neutral)\n",
    "    plt.plot(alphas[~mask],pos_gen_err[~mask],color=c_neutral)\n",
    "    \n",
    "    plt.axvline(1.45,ymin=0, ymax=1, color=\"r\", ls=\"--\")\n",
    "\n",
    "    plt.ylabel(r'$\\epsilon_g$')\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.0,2.0)\n",
    "    plt.show()\n",
    "\n",
    "    '''\n",
    "    lin_error = results[]\n",
    "    plt.plot(df_positional[df_positional[\"alpha\"]<1.25].alpha,df_positional[df_positional[\"alpha\"]<1.25].mse/2, color=c_neutral,label=r'$\\epsilon_g^{\\mathrm{minloss}}$')\n",
    "    plt.plot(df_semantic[df_semantic[\"alpha\"]>1.24].alpha,df_semantic[df_semantic[\"alpha\"]>1.24].mse/2, color=c_neutral)\n",
    "\n",
    "    plt.axvline(df_spinodal.loc[0.3].alpha_cross,ymin=0, ymax=1, color=\"g\", ls=\"--\")\n",
    "    plt.axvline(1.45,ymin=0, ymax=1, color=\"r\", ls=\"--\")\n",
    "\n",
    "    plt.xticks(ticks=[0.,.5,1.,2.] + [df_spinodal.loc[0.3].alpha_cross,1.45],labels=[str(i) for i in [0.,.5,1.,2.]]+[r\"$\\alpha_c$\",r\"$\\alpha_l$\"])\n",
    "\n",
    "\n",
    "    plt.ylabel(r'$\\epsilon_g$')\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.0,2.0)\n",
    "    plt.savefig(figure_dir / 'attention_gen_error_global_only.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "alphas = np.linspace(0.2,2,10)\n",
    "results2 = run_fig_2C(X,T,alphas)\n",
    "plot_fig2C(results,results2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FIG 3:\n",
    "FIG 3A -> ) Scaling d and n jointly for Î± = 1.5 concentrates for Î¸ and m,\n",
    "in different locations for the positional and semantic local minima each. We show 30 runs for each\n",
    "d âˆˆ [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]\n",
    "FIG 3B -> alpha vs omega color plot of delta E training\n",
    "FIG 3C ->  color map : difference in test MSE at convergence when training the attention model (13) using the Pytorch\n",
    "implementation of full-batch gradient descent initialized at Qâ‹†, and the dense linear baseline (15).\n",
    "The red dashed lines indicate the theoretical prediction â€“following from Result 4.2 and Result 15â€“\n",
    "for the threshold sample complexity Î±l(Ï‰) above which the dot-product attention (2) outperforms the\n",
    "baseline\n",
    "'''\n",
    "alpha_cross = np.sort([0.5480511155124588,0.7071599112677189,0.9277860822926854,1.2410177405149914,1.06,1.4541109691229464,1.93929723421423,1.7505718608649372,0.8180015676221762])\n",
    "omegas = np.sort([0.,.1,.2,.3,.25,.35,.425,.4,.15])\n",
    "# 9x9 grid \n",
    "def fig3BC(alphas,omegas,D=1000,seed=42):\n",
    "    results = []\n",
    "    N = int(2.2*D)\n",
    "    torch.manual_seed(seed)\n",
    "    for omega in omegas:\n",
    "        print(\"omega = \",omega)\n",
    "        print(D)\n",
    "        X, T, W_Q_teacher = generate_teacher_dataset(\n",
    "            N=N, D=D, L=L, DK=DK, omega=omega, seed=seed\n",
    "        )\n",
    "        result = run_fig_2A(X,T,lam=1e-2,WQ_teacher=W_Q_teacher,alphas = alphas,epochs=10)\n",
    "        result_lin = run_fig_2C(X,T,alphas)\n",
    "        for res, lin_res in zip(result, result_lin):\n",
    "            res['omega'] = omega\n",
    "            res['lin_gen_error'] = lin_res['linear_gen_error']\n",
    "            res['lin_train_error'] = lin_res['linear_train_error']\n",
    "        results.append(result)\n",
    "    return results # list of list of dictionaries\n",
    " \n",
    "def convert_to_df(results):\n",
    "    flat_results = [item for sublist in results for item in sublist]\n",
    "    df = pd.DataFrame(flat_results)\n",
    "    return df\n",
    "\n",
    "def plot_Fig3B(df):\n",
    "    alphas = np.array(sorted(df['alpha'].unique()))\n",
    "    omegas = np.array(sorted(df['omega'].unique()))\n",
    "\n",
    "    len_alpha = np.shape(alphas)[0]\n",
    "    len_omega = np.shape(omegas)[0]\n",
    "    delta_generalisation_error = np.array(df['delta_gen']).reshape(len_alpha,len_omega) # idk if this is correct\n",
    "\n",
    "    vmax=0.0015\n",
    "    plt.figure(figsize=(4,3.5))\n",
    "    plt.imshow(delta_generalisation_error/10 ,#/ 1000, \n",
    "               aspect='auto',  \n",
    "               cmap=cmap_uninf, \n",
    "               vmin=-vmax, vmax=vmax, \n",
    "               extent=[alphas[0], alphas[-1], omegas[0], omegas[-1]])\n",
    "    \n",
    "    plt.colorbar(label=r'$\\Delta \\epsilon_t$')\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.ylabel(r'$\\omega$')\n",
    "\n",
    "    plt.ylim(0.01,0.5)\n",
    "    plt.xlim(0,2.0)\n",
    "    plt.show()\n",
    "\n",
    "def plot_Fig3C(df):\n",
    "    alphas = np.array(sorted(df['alpha'].unique()))\n",
    "    omegas = np.array(sorted(df['omega'].unique()))\n",
    "    print(alphas)\n",
    "    print(omegas)\n",
    "\n",
    "    len_alpha = np.shape(alphas)[0]\n",
    "    len_omega = np.shape(omegas)[0]\n",
    "    sem_error_gen = np.array(df['semantic_loss_gen'])\n",
    "    lin_loss_gen = np.array(df['lin_gen_error'])\n",
    "    \n",
    "    delta_err = (lin_loss_gen-sem_error_gen).reshape(len_alpha,len_omega)\n",
    "\n",
    "    fig=plt.figure(figsize=(4,3.5))\n",
    "    vmax = 0.002\n",
    "    #plt.pcolor(alphas,omegas,delta_err/1000,vmin=-vmax,vmax=vmax,cmap=cmap_uninf)\n",
    "    plt.imshow(delta_err ,#/ 1000, \n",
    "               aspect='auto',  \n",
    "               cmap=cmap_attlin, \n",
    "               vmin=-vmax, vmax=vmax, \n",
    "               extent=[alphas[0], alphas[-1], omegas[0], omegas[-1]])\n",
    "    \n",
    "    plt.colorbar(label=r'$\\epsilon^{\\mathrm{lin}}_g-\\epsilon_g$')\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.ylabel(r'$\\omega$') \n",
    "    plt.ylim(0.01,0.5)\n",
    "    plt.xlim(0,2.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# use omega_list_short \n",
    "# make sure fig 2 plots are fine before going to this\n",
    "results = fig3BC(alpha_cross ,omegas,D=1000)\n",
    "df = convert_to_df(results)\n",
    "plot_Fig3C(df)\n",
    "plot_Fig3B(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scaling d and n jointly for Î± = 1.5 concentrates for Î¸ and\n",
    "m, in different locations for the positional and semantic local minima each. We show 30 runs for each\n",
    "d âˆˆ [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. '''\n",
    "\n",
    "def fig3A(d_list,alpha=1.5,omega=0.3):\n",
    "    results = []\n",
    "    for d in d_list:\n",
    "        n = 2.2*d\n",
    "        X, T, W_Q_teacher = generate_teacher_dataset(\n",
    "            N=n, D=d, L=L, DK=DK, omega=omega, seed=seed\n",
    "        )\n",
    "        print(alpha)\n",
    "        result = run_fig_2A(X,T,lam=1e-2,WQ_teacher=W_Q_teacher,alphas = alpha,epochs=50)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def plot_fig3A(df,D_list):\n",
    "    sigma=0.5\n",
    "    m_pos = np.array(df['m_pos'])\n",
    "    m_sem = np.array(df['m_pos'])\n",
    "    theta_pos = np.array(df['theta_pos'])\n",
    "    theta_sem = np.array(df['theta_sem'])\n",
    "    print(m_pos)\n",
    "    print(m_sem)\n",
    "    plt.scatter(m_sem/sigma**2,theta_sem/sigma**2,cmap=cmap_att, norm=colors.LogNorm())\n",
    "    plt.scatter(m_sem/sigma**2, theta_sem/sigma**2, marker='+',c=D_list, cmap=cmap_att, norm=colors.LogNorm())\n",
    "    plt.colorbar(label=r'$d$')\n",
    "    plt.scatter(m_pos/sigma**2,theta_pos/sigma**2,cmap=cmap_pos, norm=colors.LogNorm())\n",
    "    plt.scatter(m_pos/sigma**2,theta_sem/sigma**2,marker='+',c=D_list, cmap=cmap_pos, norm=colors.LogNorm())\n",
    "    plt.colorbar()\n",
    "    plt.scatter([],[],marker='+',c=c_semantic,label='semantic')\n",
    "    plt.scatter([],[],marker='+',c=c_positional,label='positional')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(r'$m/\\sigma^2$')\n",
    "    plt.ylabel(r'$\\theta/\\sigma^2$')\n",
    "\n",
    "alpha = [1.5]\n",
    "omega = 0.3\n",
    "d = np.array([10, 15, 23, 36, 56, 87, 135, 209, 323, 500])\n",
    "#results = fig3A(d,alpha,omega)\n",
    "df = convert_to_df(results)\n",
    "plot_fig3A(df,d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoperon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
